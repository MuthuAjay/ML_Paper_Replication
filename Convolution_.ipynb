{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "493342fe-0663-40d7-9772-4f2cda7b67f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dc79a6-7d03-42bb-94ad-c29209784032",
   "metadata": {},
   "outputs": [],
   "source": [
    "#layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20213033-5a50-4ebd-b929-3cf6ffdaf274",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = 3\n",
    "out_channels = 10\n",
    "kernel_size = (3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "3ad4d1ca-4e85-439d-997e-93018459209a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'out_channels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[154], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#initiate weights \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m torch\u001b[38;5;241m.\u001b[39mrandn((\u001b[43mout_channels\u001b[49m,in_channels,\u001b[38;5;241m*\u001b[39mkernel_size))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'out_channels' is not defined"
     ]
    }
   ],
   "source": [
    "#initiate weights \n",
    "torch.randn((out_channels,in_channels,*kernel_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f60574a2-04b0-43df-a725-b7e913d99d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f901cea6-8ca0-41f9-9d11-7e71b012b5c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 9.7780e-01, -2.5650e+00,  4.5908e-01],\n",
       "          [-6.5964e-01, -5.5390e-01, -2.3156e-01],\n",
       "          [ 4.4518e-01, -8.4500e-01, -5.1839e-01]],\n",
       "\n",
       "         [[-3.5526e-03,  1.5240e+00, -3.9857e-02],\n",
       "          [-7.3219e-02, -1.2916e+00,  1.7444e-01],\n",
       "          [ 7.9504e-02,  3.3009e-01, -2.5717e-02]],\n",
       "\n",
       "         [[ 1.3604e+00, -1.9060e+00,  1.0994e+00],\n",
       "          [-7.2749e-01, -2.0732e+00, -7.7894e-01],\n",
       "          [-1.4691e+00,  1.6194e-01, -7.7943e-01]],\n",
       "\n",
       "         [[-7.7918e-01,  1.1310e+00,  1.0559e+00],\n",
       "          [ 3.0552e-01,  6.0789e-02, -6.7630e-01],\n",
       "          [ 2.9063e-01, -1.1181e+00,  4.2425e-01]],\n",
       "\n",
       "         [[-2.0772e+00, -5.5466e-01, -8.3448e-01],\n",
       "          [-5.0678e-02,  9.0030e-01,  2.0078e-01],\n",
       "          [-3.3117e-01, -1.2373e+00, -4.3185e-01]],\n",
       "\n",
       "         [[-6.7557e-01, -6.3573e-01, -1.1711e+00],\n",
       "          [ 1.2524e-02,  4.4757e-01,  1.8580e-01],\n",
       "          [ 2.9108e-01, -1.2670e-01, -2.6969e-01]],\n",
       "\n",
       "         [[ 1.6968e+00, -2.0797e+00,  2.1210e-01],\n",
       "          [-6.6815e-01, -5.4668e-01, -9.9085e-01],\n",
       "          [-8.1005e-01,  3.9617e-01,  1.7161e+00]],\n",
       "\n",
       "         [[ 8.0872e-01,  1.1103e-01, -1.6423e-01],\n",
       "          [-1.0610e+00, -7.0967e-01, -3.4134e-01],\n",
       "          [-4.6644e-01, -1.2232e+00,  2.2661e-01]],\n",
       "\n",
       "         [[-1.2711e+00, -1.7267e-01,  1.4971e-01],\n",
       "          [ 7.6659e-01, -5.9829e-01, -4.8203e-01],\n",
       "          [ 2.5340e-01, -9.9574e-01, -3.0864e-01]],\n",
       "\n",
       "         [[-9.9657e-01,  7.9941e-01, -1.7721e+00],\n",
       "          [-9.3721e-02, -2.1443e-01,  8.9815e-01],\n",
       "          [-9.1318e-01,  7.5650e-01, -7.4151e-01]]],\n",
       "\n",
       "\n",
       "        [[[-3.9231e-01,  8.6213e-01, -1.4576e+00],\n",
       "          [-1.0775e+00, -7.7060e-02, -3.7614e-01],\n",
       "          [ 3.1362e-01,  1.0926e-01, -6.5698e-01]],\n",
       "\n",
       "         [[-4.0209e-01,  9.0338e-02, -7.1022e-01],\n",
       "          [-1.4903e+00,  1.7578e+00,  1.0583e+00],\n",
       "          [-1.8945e+00,  1.5391e+00,  8.5541e-02]],\n",
       "\n",
       "         [[ 1.4232e+00,  5.7000e-01,  1.1153e+00],\n",
       "          [-8.5711e-02, -1.1704e+00, -1.9581e+00],\n",
       "          [-1.4984e+00,  7.5698e-01, -1.0016e-01]],\n",
       "\n",
       "         [[ 7.5214e-01,  1.1323e+00, -1.8768e-01],\n",
       "          [ 1.2969e-01, -2.7266e+00,  3.5129e-01],\n",
       "          [-1.8195e+00, -6.7828e-01,  1.1822e+00]],\n",
       "\n",
       "         [[ 1.6799e+00, -6.6902e-01,  4.4807e-01],\n",
       "          [ 3.8284e-01,  5.7975e-02,  1.8551e+00],\n",
       "          [-6.4531e-01, -4.3262e-01, -8.9703e-01]],\n",
       "\n",
       "         [[-1.1525e+00, -1.5554e+00,  6.5037e-01],\n",
       "          [-1.6790e+00,  2.3435e-01, -8.9846e-01],\n",
       "          [ 1.1259e+00,  4.8806e-01,  1.0766e+00]],\n",
       "\n",
       "         [[-3.9104e-01,  1.4606e+00, -2.9938e-02],\n",
       "          [-1.0179e+00,  1.4786e+00,  5.9936e-01],\n",
       "          [-6.4494e-01,  1.0229e+00,  1.7309e-01]],\n",
       "\n",
       "         [[-1.5778e+00, -1.6808e-01, -8.3723e-01],\n",
       "          [ 1.9848e-01,  1.3159e+00,  7.4724e-01],\n",
       "          [ 1.5049e+00,  4.2412e-01, -8.0392e-01]],\n",
       "\n",
       "         [[-1.9508e+00, -9.4834e-01,  8.7768e-01],\n",
       "          [-2.2488e-01, -6.3378e-01, -1.5254e+00],\n",
       "          [-4.9328e-01,  9.7524e-01,  1.4350e+00]],\n",
       "\n",
       "         [[-7.1607e-01, -1.7412e+00, -3.1053e-01],\n",
       "          [ 4.7178e-01, -7.9713e-01,  5.4316e-02],\n",
       "          [ 3.5770e-01, -7.2362e-01,  1.2535e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 8.4958e-01,  1.8575e-01, -7.6954e-01],\n",
       "          [ 2.3676e+00,  4.4273e-01, -5.4819e-01],\n",
       "          [ 1.8720e-01, -9.6099e-01, -1.6356e+00]],\n",
       "\n",
       "         [[ 5.3186e-01,  8.3375e-01,  7.9204e-01],\n",
       "          [-3.5692e-01,  2.2007e+00,  2.0460e-01],\n",
       "          [ 1.8069e-02, -1.9236e-01, -1.7429e+00]],\n",
       "\n",
       "         [[-2.2009e-01,  6.0219e-01,  5.7700e-01],\n",
       "          [-8.0377e-02,  5.7839e-01,  1.5591e+00],\n",
       "          [-3.9254e-01, -2.0111e-01, -5.6723e-01]],\n",
       "\n",
       "         [[ 1.6243e+00,  1.7092e+00,  1.0210e-01],\n",
       "          [-2.9907e-01, -2.3714e+00,  8.1423e-01],\n",
       "          [-8.5684e-01,  1.0366e+00,  8.8860e-01]],\n",
       "\n",
       "         [[-1.0706e+00, -2.0783e-01,  1.1778e+00],\n",
       "          [-5.7917e-02,  2.1403e+00, -6.1302e-01],\n",
       "          [ 1.4561e+00,  1.9735e+00,  1.4516e-01]],\n",
       "\n",
       "         [[-1.9807e+00,  4.2515e-01, -2.8448e-01],\n",
       "          [ 1.0945e+00, -2.1964e+00,  3.0851e-01],\n",
       "          [ 7.4624e-01, -4.5006e-01,  7.7410e-01]],\n",
       "\n",
       "         [[-1.3292e+00,  5.5455e-01,  8.8837e-01],\n",
       "          [-7.5886e-01,  5.3500e-01, -6.9637e-02],\n",
       "          [ 2.3091e-01, -1.2131e+00, -1.8689e+00]],\n",
       "\n",
       "         [[-6.8063e-01, -1.0929e+00, -6.4445e-02],\n",
       "          [ 7.9028e-02,  4.5680e-01, -2.3744e-01],\n",
       "          [-2.4834e-01, -3.4277e-01,  1.8660e+00]],\n",
       "\n",
       "         [[ 9.4297e-01, -9.8237e-01, -1.8969e-01],\n",
       "          [ 3.7869e-01,  9.8745e-02, -1.7322e-02],\n",
       "          [-6.6980e-01,  1.0942e+00,  7.7850e-01]],\n",
       "\n",
       "         [[ 3.2471e-01,  6.1270e-01,  5.9507e-01],\n",
       "          [ 9.0011e-01, -1.4754e-01,  4.9067e-01],\n",
       "          [-9.4819e-02, -2.0762e+00, -9.6591e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.1747e+00, -4.2560e-01, -4.5402e-01],\n",
       "          [ 1.5460e-01,  1.7526e-01,  1.4206e-01],\n",
       "          [ 3.6370e-01,  2.6730e-01,  2.5340e+00]],\n",
       "\n",
       "         [[-5.4923e-01,  2.3713e-01,  6.5169e-01],\n",
       "          [-8.1815e-01, -6.4486e-02,  1.8986e-01],\n",
       "          [ 1.8971e+00,  8.7930e-01, -2.4784e+00]],\n",
       "\n",
       "         [[ 1.0084e-01,  4.2475e-01,  4.8285e-01],\n",
       "          [ 3.6016e-01,  1.3386e+00, -1.6173e-01],\n",
       "          [-2.9814e-02, -1.0409e+00, -3.5013e-01]],\n",
       "\n",
       "         [[ 7.7065e-01, -8.3134e-01, -1.3453e+00],\n",
       "          [-7.2530e-01,  9.7194e-01, -9.0787e-02],\n",
       "          [-2.0235e+00,  1.5653e+00, -2.6877e-01]],\n",
       "\n",
       "         [[ 1.2485e+00, -3.9795e-01, -1.5919e-01],\n",
       "          [ 2.6735e-01, -2.0190e+00, -3.8691e-01],\n",
       "          [-6.2711e-01,  1.6350e+00,  2.0355e-01]],\n",
       "\n",
       "         [[ 2.1852e-01, -3.6355e-01, -5.6348e-01],\n",
       "          [ 1.9488e+00,  7.8300e-01, -4.9523e-01],\n",
       "          [-8.4050e-01, -1.7890e-01,  6.9841e-01]],\n",
       "\n",
       "         [[ 1.3221e+00,  1.8644e-01, -1.8587e+00],\n",
       "          [-1.7950e-01,  1.1602e+00, -3.0134e-01],\n",
       "          [-8.3373e-01, -1.4052e+00,  9.1413e-02]],\n",
       "\n",
       "         [[-1.2352e+00,  3.7902e-01, -2.7876e-01],\n",
       "          [ 4.0595e-01,  1.3198e-01,  7.7679e-02],\n",
       "          [ 1.2477e+00, -4.1230e-01,  6.3928e-01]],\n",
       "\n",
       "         [[ 8.1662e-01,  7.4407e-01,  1.6613e+00],\n",
       "          [-1.2865e+00, -1.0797e+00,  7.7057e-01],\n",
       "          [ 1.1666e+00, -6.0166e-01,  4.2125e-01]],\n",
       "\n",
       "         [[ 3.6617e-01,  6.9594e-02,  1.0071e+00],\n",
       "          [ 1.2671e+00, -2.1927e-01, -3.4618e-01],\n",
       "          [-4.4086e-01, -6.1412e-01, -1.2728e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 8.4780e-01, -4.5298e-02, -3.2474e+00],\n",
       "          [ 4.1966e-01, -1.4932e+00,  1.3189e-01],\n",
       "          [ 5.3677e-01,  7.8566e-01, -1.3533e+00]],\n",
       "\n",
       "         [[ 1.2021e+00,  3.5809e-02, -2.4122e-01],\n",
       "          [ 1.4589e-01, -8.8673e-01, -4.3726e-01],\n",
       "          [ 5.5269e-01,  2.2418e-01,  1.9859e-01]],\n",
       "\n",
       "         [[-1.2937e-01, -3.8708e-01,  1.3283e+00],\n",
       "          [-4.1631e-01, -5.6394e-01, -6.3042e-01],\n",
       "          [-1.4273e+00,  1.6659e+00,  8.0571e-01]],\n",
       "\n",
       "         [[ 4.5548e-01,  9.6701e-01, -1.1683e+00],\n",
       "          [ 1.4089e+00, -3.5938e-01,  1.0492e+00],\n",
       "          [-1.9895e+00, -9.6766e-01,  8.7700e-01]],\n",
       "\n",
       "         [[ 4.3017e-01, -5.6770e-01,  4.8052e-01],\n",
       "          [ 9.3110e-01, -4.8683e-01,  1.3745e+00],\n",
       "          [-1.3637e-01, -2.8441e-01, -1.7132e-01]],\n",
       "\n",
       "         [[ 1.4167e+00,  7.4514e-01,  1.7287e+00],\n",
       "          [ 8.9548e-01,  2.0030e+00,  1.0817e+00],\n",
       "          [-7.0981e-01, -2.6074e-01,  1.0094e-01]],\n",
       "\n",
       "         [[ 1.9633e-01, -1.7082e-01, -3.0709e-02],\n",
       "          [ 1.6255e-01,  4.0322e-01,  4.6458e-01],\n",
       "          [ 4.8435e-01,  4.4237e-01, -8.2266e-01]],\n",
       "\n",
       "         [[ 6.3084e-01, -3.4202e-02,  1.2810e+00],\n",
       "          [ 3.9275e-01,  1.2694e+00, -1.8945e-01],\n",
       "          [ 4.3117e-02, -1.6667e-02,  1.6746e+00]],\n",
       "\n",
       "         [[-6.2813e-01, -8.0501e-01, -1.6391e+00],\n",
       "          [ 9.4577e-01, -1.3011e-01,  1.4257e+00],\n",
       "          [ 4.5517e-01,  1.3918e-01, -1.9754e-01]],\n",
       "\n",
       "         [[ 2.6017e-01,  6.9362e-01, -9.7976e-01],\n",
       "          [ 8.7087e-01, -1.0825e+00, -8.3767e-01],\n",
       "          [-5.0333e-01, -2.2123e-01,  6.9237e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 4.7731e-01,  5.0448e-01,  1.1745e-01],\n",
       "          [ 1.2211e+00,  7.4093e-02,  1.0079e+00],\n",
       "          [ 1.6268e+00, -1.6965e+00,  1.2330e+00]],\n",
       "\n",
       "         [[-1.3548e+00,  9.7836e-02,  2.0444e-01],\n",
       "          [-3.6415e+00,  5.9645e-02, -3.4721e-01],\n",
       "          [ 9.5720e-01, -1.4037e-01,  1.0780e+00]],\n",
       "\n",
       "         [[ 3.4309e-01,  6.7659e-01,  1.4405e+00],\n",
       "          [-1.7874e+00,  5.7292e-01, -2.4999e+00],\n",
       "          [ 1.8482e+00,  2.0433e-01, -2.0712e-01]],\n",
       "\n",
       "         [[-1.0172e+00,  8.7909e-02,  9.0048e-01],\n",
       "          [ 1.0536e-01,  1.0372e+00,  1.3149e+00],\n",
       "          [-1.7194e-01,  1.7766e+00, -1.4032e+00]],\n",
       "\n",
       "         [[-3.7494e-01, -5.1738e-01,  7.9154e-01],\n",
       "          [-3.6855e-01, -3.6401e-03,  1.0170e-01],\n",
       "          [ 8.5112e-01,  1.9607e+00, -6.2504e-01]],\n",
       "\n",
       "         [[-2.9941e-01,  2.2774e+00, -9.1008e-01],\n",
       "          [-5.2405e-01, -3.6268e-01, -2.2172e-01],\n",
       "          [-2.0687e-01, -2.8335e-02, -5.8224e-02]],\n",
       "\n",
       "         [[-4.0245e-01, -1.5042e+00, -1.4914e+00],\n",
       "          [ 1.9752e-01, -1.3503e+00,  8.8613e-01],\n",
       "          [-2.0291e+00,  1.1260e+00,  1.9709e-01]],\n",
       "\n",
       "         [[ 1.1811e+00, -3.0657e-01,  9.4721e-01],\n",
       "          [ 6.9967e-01,  9.9961e-01,  6.8068e-01],\n",
       "          [-5.9039e-01,  2.2795e-01,  8.9276e-01]],\n",
       "\n",
       "         [[-6.8186e-01, -5.9685e-01, -1.2926e+00],\n",
       "          [-1.7761e+00, -2.3266e+00, -9.5024e-01],\n",
       "          [ 9.0948e-01,  3.5749e-01, -1.0171e+00]],\n",
       "\n",
       "         [[ 4.5088e-01, -9.7975e-01,  1.5026e-01],\n",
       "          [-1.0505e+00,  1.0358e-01, -1.7815e+00],\n",
       "          [ 9.5531e-01, -2.9162e-01,  6.6622e-01]]],\n",
       "\n",
       "\n",
       "        [[[-2.3899e-01,  3.3410e-01,  7.7633e-01],\n",
       "          [ 4.2046e-01, -6.4543e-02, -3.3790e-01],\n",
       "          [ 5.3209e-01,  1.7383e+00,  1.8069e-01]],\n",
       "\n",
       "         [[ 1.3772e+00,  6.6000e-01, -2.4446e-01],\n",
       "          [-5.4433e-01, -9.4188e-01,  9.1498e-01],\n",
       "          [ 1.3651e+00,  2.7834e+00,  1.2839e+00]],\n",
       "\n",
       "         [[ 1.5013e+00,  6.3913e-01, -6.2944e-01],\n",
       "          [-1.3150e+00, -5.5057e-01, -2.6537e+00],\n",
       "          [ 6.2545e-01, -5.7368e-01,  4.5981e-01]],\n",
       "\n",
       "         [[ 9.8926e-01,  5.2052e-01,  1.2061e-01],\n",
       "          [ 1.1248e+00,  6.3503e-01, -2.2054e+00],\n",
       "          [ 1.1140e+00,  1.3221e-01,  3.3513e+00]],\n",
       "\n",
       "         [[-7.0588e-02, -8.4039e-01, -1.2482e+00],\n",
       "          [ 5.4157e-01, -2.1502e+00,  8.0224e-01],\n",
       "          [ 1.2888e+00,  2.1760e+00, -1.1969e+00]],\n",
       "\n",
       "         [[-6.4706e-02,  9.3014e-01, -6.3661e-01],\n",
       "          [ 6.5049e-01, -1.3806e-01,  1.4330e+00],\n",
       "          [ 1.0843e+00,  1.3853e+00, -2.8494e-01]],\n",
       "\n",
       "         [[ 2.0614e+00, -2.3400e-01,  2.6194e-01],\n",
       "          [-9.4293e-01, -1.4669e+00,  9.2132e-01],\n",
       "          [ 2.0659e+00,  1.0189e+00, -2.7062e-01]],\n",
       "\n",
       "         [[-9.9972e-02,  2.7575e+00,  9.0383e-02],\n",
       "          [ 1.5901e+00,  2.6096e-01, -3.2143e-01],\n",
       "          [-6.8552e-01, -3.2434e-01, -8.3163e-01]],\n",
       "\n",
       "         [[-5.8946e-01, -5.3346e-01,  2.2842e-01],\n",
       "          [-5.5020e-01, -1.4390e+00, -2.1511e+00],\n",
       "          [ 1.1634e+00,  8.4600e-01, -1.9033e-01]],\n",
       "\n",
       "         [[ 6.1080e-02, -7.2978e-01,  1.0559e+00],\n",
       "          [-9.3972e-01, -5.9713e-01, -1.7937e-02],\n",
       "          [-5.3929e-01, -8.4886e-01, -4.5113e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.4504e+00, -8.7072e-01, -2.1359e-01],\n",
       "          [-1.8055e+00,  4.9722e-01, -1.3768e+00],\n",
       "          [-1.0072e+00,  2.1886e+00, -1.4449e+00]],\n",
       "\n",
       "         [[-5.1253e-02,  1.6089e-01, -1.1697e+00],\n",
       "          [ 1.0656e+00, -1.5619e+00,  5.7924e-01],\n",
       "          [ 5.0106e-01,  2.7362e-01,  9.2767e-01]],\n",
       "\n",
       "         [[-4.4680e-02,  2.8722e+00, -5.4644e-01],\n",
       "          [ 2.8923e-01, -7.7451e-01,  1.3310e-01],\n",
       "          [ 3.4095e-01, -1.2440e+00, -1.4891e+00]],\n",
       "\n",
       "         [[ 5.6821e-01,  6.7906e-02, -1.4356e-01],\n",
       "          [-2.0281e+00,  7.4159e-01,  2.5305e-01],\n",
       "          [ 1.0920e-01,  8.4657e-01,  6.4680e-01]],\n",
       "\n",
       "         [[ 1.0012e+00,  4.8114e-02,  2.5242e+00],\n",
       "          [-5.0762e-01, -1.5390e+00, -1.1294e-02],\n",
       "          [ 8.7880e-01,  8.6050e-01,  6.6704e-01]],\n",
       "\n",
       "         [[ 2.3861e-01,  1.4736e+00,  7.6427e-01],\n",
       "          [ 1.3102e+00,  1.1213e+00,  1.7114e+00],\n",
       "          [-1.0429e+00,  3.5004e-01, -1.5074e+00]],\n",
       "\n",
       "         [[-3.6582e-01, -7.6581e-01,  3.1865e-01],\n",
       "          [ 4.2277e-01,  8.7625e-01, -1.3144e-01],\n",
       "          [-9.7144e-01, -5.6758e-01,  3.7413e-01]],\n",
       "\n",
       "         [[ 9.9202e-01,  5.2221e-01, -4.8835e-02],\n",
       "          [ 6.0394e-01, -5.0513e-02, -2.9027e-01],\n",
       "          [-2.1843e+00,  1.6751e-01,  9.2870e-01]],\n",
       "\n",
       "         [[-1.9540e+00, -2.0874e+00,  4.8728e-01],\n",
       "          [-1.4005e+00,  5.4142e-01, -1.2064e+00],\n",
       "          [-1.2783e+00,  1.2206e+00,  9.9833e-01]],\n",
       "\n",
       "         [[-4.7244e-01, -1.0274e+00, -1.0191e+00],\n",
       "          [-1.5570e+00,  4.0104e-01, -1.0136e+00],\n",
       "          [ 9.1807e-01,  2.6729e-01,  1.6407e-01]]],\n",
       "\n",
       "\n",
       "        [[[-8.4223e-01, -1.3180e-01, -1.8624e+00],\n",
       "          [-1.2040e+00, -6.2670e-02,  2.1946e-01],\n",
       "          [ 4.6246e-01,  6.5794e-01, -4.4762e-01]],\n",
       "\n",
       "         [[-2.5298e-01,  8.3632e-01,  4.2639e-01],\n",
       "          [-5.0009e-01,  1.3572e-01, -1.7817e+00],\n",
       "          [-1.4803e+00, -5.9065e-01,  1.9647e+00]],\n",
       "\n",
       "         [[ 1.4616e+00, -1.5137e+00, -4.9388e-01],\n",
       "          [-5.3378e-01,  2.0092e-01, -4.6226e-01],\n",
       "          [ 1.3968e+00, -6.0466e-01,  1.7254e-01]],\n",
       "\n",
       "         [[-1.6694e+00, -4.5276e-01, -9.2412e-01],\n",
       "          [ 1.5472e+00,  6.5482e-01,  9.3376e-02],\n",
       "          [ 9.2521e-01, -6.9088e-01, -7.0034e-01]],\n",
       "\n",
       "         [[ 5.5195e-01,  7.4898e-01,  1.0013e+00],\n",
       "          [ 1.2604e+00, -7.3150e-01, -1.4311e+00],\n",
       "          [-1.2455e+00, -4.8584e-01,  1.4119e+00]],\n",
       "\n",
       "         [[ 1.3775e+00, -8.2350e-01,  1.2574e+00],\n",
       "          [ 2.1889e+00,  4.5394e-01,  1.0184e+00],\n",
       "          [ 6.9584e-01, -2.1746e-01,  3.5611e-01]],\n",
       "\n",
       "         [[-5.3984e-01, -6.5993e-01,  1.1350e-01],\n",
       "          [-6.9771e-01, -1.4789e+00,  6.0285e-01],\n",
       "          [-8.2580e-01, -3.8209e-01, -1.8263e+00]],\n",
       "\n",
       "         [[-9.0881e-01, -7.3949e-01,  1.8912e+00],\n",
       "          [-1.6208e+00,  2.0341e-01, -3.6263e-02],\n",
       "          [ 4.9120e-02, -1.3371e-01, -1.0308e+00]],\n",
       "\n",
       "         [[ 1.8791e+00,  1.2149e+00,  3.1604e-01],\n",
       "          [-9.7194e-01, -6.6176e-01,  1.0261e+00],\n",
       "          [-4.9991e-01,  1.0051e+00,  7.7240e-01]],\n",
       "\n",
       "         [[-1.0004e+00, -3.5420e-01,  3.3999e-01],\n",
       "          [ 4.7367e-01,  1.0223e+00,  6.9914e-02],\n",
       "          [ 1.2892e+00, -4.6032e-01,  6.1118e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 3.5249e-01,  2.4317e-02,  3.9483e-01],\n",
       "          [-2.2192e-01,  5.9960e-02,  1.4962e+00],\n",
       "          [ 2.5916e-02,  1.6805e+00,  2.0226e+00]],\n",
       "\n",
       "         [[-5.2489e-01, -1.4550e+00, -9.5049e-01],\n",
       "          [ 2.3029e+00, -1.8636e+00, -5.6529e-01],\n",
       "          [ 1.2639e+00, -4.0807e-01,  1.1144e+00]],\n",
       "\n",
       "         [[-1.2409e+00,  2.1807e-01,  1.3706e-01],\n",
       "          [ 3.9387e-01, -2.5350e-01,  1.5952e+00],\n",
       "          [-1.0073e+00,  1.5699e+00, -7.2981e-01]],\n",
       "\n",
       "         [[ 1.7839e+00, -1.7170e+00, -9.5128e-01],\n",
       "          [-3.7996e-01,  1.2558e+00, -7.6364e-01],\n",
       "          [-2.1805e-01,  1.9197e-01, -1.2133e+00]],\n",
       "\n",
       "         [[-1.0928e+00,  9.4001e-01,  2.9269e-01],\n",
       "          [ 2.8527e-01,  2.1701e-01, -2.1547e+00],\n",
       "          [-7.4765e-01,  8.8155e-01,  5.9543e-01]],\n",
       "\n",
       "         [[-9.6954e-01, -1.3183e+00,  4.4663e-01],\n",
       "          [-1.0926e+00, -2.4921e-02,  4.1741e-01],\n",
       "          [ 1.1882e+00, -8.8586e-01,  4.5105e-01]],\n",
       "\n",
       "         [[-8.7912e-02,  1.3588e+00, -9.8946e-01],\n",
       "          [ 6.3915e-01, -5.1182e-01,  1.4329e-01],\n",
       "          [ 8.6765e-01,  2.0839e+00,  2.1895e-01]],\n",
       "\n",
       "         [[ 1.4857e+00, -5.6099e-01,  4.3836e-01],\n",
       "          [ 7.9545e-01, -5.1900e-01,  8.1541e-01],\n",
       "          [-1.3083e+00, -7.5660e-03, -7.0093e-01]],\n",
       "\n",
       "         [[ 6.0475e-01,  1.2928e+00, -8.3413e-01],\n",
       "          [ 8.1763e-02,  6.3851e-03,  1.8984e+00],\n",
       "          [-3.0016e-01, -3.3510e-02, -8.0519e-01]],\n",
       "\n",
       "         [[-1.4619e+00, -4.5383e-01, -3.8037e-01],\n",
       "          [-1.1793e+00,  1.5789e+00,  1.4968e+00],\n",
       "          [ 5.9632e-01, -7.3286e-01,  4.9265e-01]]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_channels = 10\n",
    "out_channels = 10\n",
    "kernel_size = (3,3)\n",
    "\n",
    "#initialize weights\n",
    "torch.randn((out_channels, in_channels, *kernel_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf328985-1e6f-4272-95a8-ae5f6c12924d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95fce98b-c85a-4414-85c5-21bd19e6c29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 4, 4])\n",
      "1 3\n",
      "tensor([[[[ 1.6427,  1.0322],\n",
      "          [ 0.4579,  0.0647]],\n",
      "\n",
      "         [[ 0.1017,  0.0574],\n",
      "          [-1.0971, -0.0121]],\n",
      "\n",
      "         [[ 0.5391, -0.7971],\n",
      "          [-0.0425, -0.1239]]]])\n",
      "tensor([[[ 1.6427,  1.0322,  0.4579,  0.0647],\n",
      "         [ 0.1017,  0.0574, -1.0971, -0.0121],\n",
      "         [ 0.5391, -0.7971, -0.0425, -0.1239]]])\n",
      "tensor([[[[ 0.9477, -2.1046],\n",
      "          [-0.8918,  1.1653]],\n",
      "\n",
      "         [[ 0.3947,  0.0235],\n",
      "          [-0.5901, -1.0602]],\n",
      "\n",
      "         [[-0.5812,  2.2962],\n",
      "          [ 0.4634,  0.1835]]]])\n",
      "tensor([[[ 0.9477, -2.1046, -0.8918,  1.1653],\n",
      "         [ 0.3947,  0.0235, -0.5901, -1.0602],\n",
      "         [-0.5812,  2.2962,  0.4634,  0.1835]]])\n",
      "tensor([[[[ 1.4651,  0.3512],\n",
      "          [-0.8567,  0.9281]],\n",
      "\n",
      "         [[-0.3210,  1.1427],\n",
      "          [ 1.3828,  0.6929]],\n",
      "\n",
      "         [[-1.3889,  1.3208],\n",
      "          [ 0.9103,  2.2272]]]])\n",
      "tensor([[[ 1.4651,  0.3512, -0.8567,  0.9281],\n",
      "         [-0.3210,  1.1427,  1.3828,  0.6929],\n",
      "         [-1.3889,  1.3208,  0.9103,  2.2272]]])\n",
      "tensor([[[[ 2.1653,  1.0230],\n",
      "          [-1.0879,  1.6244]],\n",
      "\n",
      "         [[ 0.8179,  1.7951],\n",
      "          [ 0.2702,  0.6716]],\n",
      "\n",
      "         [[ 1.7785, -1.3245],\n",
      "          [ 1.6679,  0.3923]]]])\n",
      "tensor([[[ 2.1653,  1.0230, -1.0879,  1.6244],\n",
      "         [ 0.8179,  1.7951,  0.2702,  0.6716],\n",
      "         [ 1.7785, -1.3245,  1.6679,  0.3923]]])\n",
      "tensor([[[[-0.6720,  0.5533],\n",
      "          [-0.2753,  1.3022]],\n",
      "\n",
      "         [[-2.1193,  0.9050],\n",
      "          [ 0.0952,  1.8039]],\n",
      "\n",
      "         [[ 1.1057,  1.5228],\n",
      "          [-0.4256, -0.3552]]]])\n",
      "tensor([[[-0.6720,  0.5533, -0.2753,  1.3022],\n",
      "         [-2.1193,  0.9050,  0.0952,  1.8039],\n",
      "         [ 1.1057,  1.5228, -0.4256, -0.3552]]])\n",
      "tensor([[[[ 0.5448, -0.0686],\n",
      "          [ 0.9019, -0.3466]],\n",
      "\n",
      "         [[-0.3549, -0.1906],\n",
      "          [ 1.7464,  0.0165]],\n",
      "\n",
      "         [[-0.9150,  1.3378],\n",
      "          [-0.2868,  1.7821]]]])\n",
      "tensor([[[ 0.5448, -0.0686,  0.9019, -0.3466],\n",
      "         [-0.3549, -0.1906,  1.7464,  0.0165],\n",
      "         [-0.9150,  1.3378, -0.2868,  1.7821]]])\n",
      "tensor([[[[-1.6824, -0.5103],\n",
      "          [-0.7578, -2.4150]],\n",
      "\n",
      "         [[ 0.6341, -0.9737],\n",
      "          [ 1.8264,  0.3372]],\n",
      "\n",
      "         [[ 0.8984, -0.6871],\n",
      "          [ 0.5191,  1.2115]]]])\n",
      "tensor([[[-1.6824, -0.5103, -0.7578, -2.4150],\n",
      "         [ 0.6341, -0.9737,  1.8264,  0.3372],\n",
      "         [ 0.8984, -0.6871,  0.5191,  1.2115]]])\n",
      "tensor([[[[ 0.3585, -0.5304],\n",
      "          [ 0.0503,  1.2618]],\n",
      "\n",
      "         [[ 1.4221,  0.4497],\n",
      "          [-1.2550,  0.3158]],\n",
      "\n",
      "         [[-1.4809, -1.4683],\n",
      "          [-0.2284, -1.1011]]]])\n",
      "tensor([[[ 0.3585, -0.5304,  0.0503,  1.2618],\n",
      "         [ 1.4221,  0.4497, -1.2550,  0.3158],\n",
      "         [-1.4809, -1.4683, -0.2284, -1.1011]]])\n",
      "tensor([[[[-1.2166, -1.3246],\n",
      "          [-0.3833,  1.8199]],\n",
      "\n",
      "         [[-0.2004,  0.2013],\n",
      "          [ 0.0908,  0.9518]],\n",
      "\n",
      "         [[-0.1496,  0.9476],\n",
      "          [ 0.9773, -1.0187]]]])\n",
      "tensor([[[-1.2166, -1.3246, -0.3833,  1.8199],\n",
      "         [-0.2004,  0.2013,  0.0908,  0.9518],\n",
      "         [-0.1496,  0.9476,  0.9773, -1.0187]]])\n",
      "tensor([[[[ 2.3357,  1.1132],\n",
      "          [ 0.4082, -0.1625]],\n",
      "\n",
      "         [[-0.7379, -0.6752],\n",
      "          [-0.7026,  0.1586]],\n",
      "\n",
      "         [[-1.0991, -1.5041],\n",
      "          [ 0.0335, -0.7383]]]])\n",
      "tensor([[[ 2.3357,  1.1132,  0.4082, -0.1625],\n",
      "         [-0.7379, -0.6752, -0.7026,  0.1586],\n",
      "         [-1.0991, -1.5041,  0.0335, -0.7383]]])\n",
      "tensor([[[[ 0.9530, -2.0049],\n",
      "          [ 0.4701,  0.2581]],\n",
      "\n",
      "         [[-0.1511, -0.8834],\n",
      "          [-1.8104, -1.3224]],\n",
      "\n",
      "         [[-0.0907, -0.6193],\n",
      "          [ 0.3228,  0.8344]]]])\n",
      "tensor([[[ 0.9530, -2.0049,  0.4701,  0.2581],\n",
      "         [-0.1511, -0.8834, -1.8104, -1.3224],\n",
      "         [-0.0907, -0.6193,  0.3228,  0.8344]]])\n",
      "tensor([[[[ 0.3860,  2.7921],\n",
      "          [-1.9124,  0.7244]],\n",
      "\n",
      "         [[-0.3885, -0.4735],\n",
      "          [ 0.8584,  1.1992]],\n",
      "\n",
      "         [[ 0.5890,  0.9459],\n",
      "          [ 0.7011,  1.5670]]]])\n",
      "tensor([[[ 0.3860,  2.7921, -1.9124,  0.7244],\n",
      "         [-0.3885, -0.4735,  0.8584,  1.1992],\n",
      "         [ 0.5890,  0.9459,  0.7011,  1.5670]]])\n",
      "tensor([[[[ 0.8206, -0.5269],\n",
      "          [ 1.7597, -2.0632]],\n",
      "\n",
      "         [[ 0.4803,  0.9165],\n",
      "          [ 0.1604,  0.0124]],\n",
      "\n",
      "         [[-0.5255,  1.2896],\n",
      "          [-0.5227, -0.4895]]]])\n",
      "tensor([[[ 0.8206, -0.5269,  1.7597, -2.0632],\n",
      "         [ 0.4803,  0.9165,  0.1604,  0.0124],\n",
      "         [-0.5255,  1.2896, -0.5227, -0.4895]]])\n",
      "tensor([[[[ 0.2981,  1.1684],\n",
      "          [ 1.2933, -0.1742]],\n",
      "\n",
      "         [[-1.7723, -0.2643],\n",
      "          [ 1.0605, -1.6215]],\n",
      "\n",
      "         [[-1.2780,  0.2157],\n",
      "          [ 0.4775,  0.4433]]]])\n",
      "tensor([[[ 0.2981,  1.1684,  1.2933, -0.1742],\n",
      "         [-1.7723, -0.2643,  1.0605, -1.6215],\n",
      "         [-1.2780,  0.2157,  0.4775,  0.4433]]])\n",
      "tensor([[[[ 1.6829,  0.4802],\n",
      "          [ 1.6050, -2.0428]],\n",
      "\n",
      "         [[-1.2653, -0.0245],\n",
      "          [ 1.1929,  0.0497]],\n",
      "\n",
      "         [[-1.1752, -0.2253],\n",
      "          [ 0.0317,  0.3441]]]])\n",
      "tensor([[[ 1.6829,  0.4802,  1.6050, -2.0428],\n",
      "         [-1.2653, -0.0245,  1.1929,  0.0497],\n",
      "         [-1.1752, -0.2253,  0.0317,  0.3441]]])\n",
      "tensor([[[[-2.5072e-01,  1.5644e+00],\n",
      "          [ 7.3218e-01, -7.1910e-03]],\n",
      "\n",
      "         [[ 5.9874e-01, -7.5569e-01],\n",
      "          [-1.0069e+00,  8.5343e-01]],\n",
      "\n",
      "         [[-3.7471e-01, -3.5897e-01],\n",
      "          [-1.3070e-03,  8.7231e-01]]]])\n",
      "tensor([[[-2.5072e-01,  1.5644e+00,  7.3218e-01, -7.1910e-03],\n",
      "         [ 5.9874e-01, -7.5569e-01, -1.0069e+00,  8.5343e-01],\n",
      "         [-3.7471e-01, -3.5897e-01, -1.3070e-03,  8.7231e-01]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.6427,  1.1653,  1.4651,  2.1653],\n",
       "          [ 1.3022,  0.9019, -0.5103,  1.2618],\n",
       "          [ 1.8199,  2.3357,  0.9530,  2.7921],\n",
       "          [ 1.7597,  1.2933,  1.6829,  1.5644]],\n",
       "\n",
       "         [[ 0.1017,  0.3947,  1.3828,  1.7951],\n",
       "          [ 1.8039,  1.7464,  1.8264,  1.4221],\n",
       "          [ 0.9518,  0.1586, -0.1511,  1.1992],\n",
       "          [ 0.9165,  1.0605,  1.1929,  0.8534]],\n",
       "\n",
       "         [[ 0.5391,  2.2962,  2.2272,  1.7785],\n",
       "          [ 1.5228,  1.7821,  1.2115, -0.2284],\n",
       "          [ 0.9773,  0.0335,  0.8344,  1.5670],\n",
       "          [ 1.2896,  0.4775,  0.3441,  0.8723]]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MaxPoolLayer:\n",
    "    def __init__(self, size):\n",
    "        self.last_input = None\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.last_input = x\n",
    "        batch_size, channels, height, width = x.shape\n",
    "        output_height = height // self.size\n",
    "        output_width = width // self.size\n",
    "        output = torch.zeros((batch_size, channels, output_height, output_width))\n",
    "        print(output.shape)\n",
    "        print(batch_size, channels)\n",
    "\n",
    "        for i in range(output_height):\n",
    "            for j in range(output_width):\n",
    "                region = x[:, :, i * self.size:(i + 1) * self.size, j * self.size:(j + 1) * self.size].clone()\n",
    "                print(region)\n",
    "                print(region.view(batch_size, channels, -1))\n",
    "                output[:, :, i, j] = torch.max(region.view(batch_size, channels, -1), dim=2)[0]\n",
    "\n",
    "        return output\n",
    "\n",
    "maxp = MaxPoolLayer(2)\n",
    "maxp.forward(torch.randn(1,3,8,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a7e1fda-5334-41e4-a95a-65b7fde47e18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[ 0.5773,  0.2666],\n",
       "           [-0.7162, -2.4324]],\n",
       " \n",
       "          [[ 1.1050, -0.5656],\n",
       "           [-0.7531, -0.5193]],\n",
       " \n",
       "          [[ 1.1093,  0.4929],\n",
       "           [ 0.8521,  0.3785]]]]),\n",
       " tensor([[[ 1.6661, -0.1036, -1.9657, -0.7794],\n",
       "          [ 0.5083, -0.5641, -0.9532,  2.4542],\n",
       "          [ 1.8015,  1.1530,  0.6015, -1.0125]]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(1,3,2,2),  torch.randn(1,3,2,2).view(1,3,-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2374c6e-939c-4dbf-8634-0c68e5539558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([[0.7720, 0.4830, 1.1073, 1.2307]]),\n",
       "indices=tensor([[0, 1, 1, 2]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.randn(1,3,2,2).view(1,3,-1) , dim = 1\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b24f2561-7283-4bca-ad0c-e39d4343f627",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = torch.zeros(1,3,4,4)\n",
    "out[:,:,1,1] = torch.max(torch.randn(1,3,2,2).view(1,3,-1) , dim = 2\n",
    "         )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6188f08c-c9cb-4424-a2e7-407d27dae6af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 1.4373, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.7581, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.8876, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000]]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7bf56d2-dd9c-4691-9e52-bf07f83ad782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0., 0.],\n",
       "          [0., 1., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0., 0.],\n",
       "          [0., 1., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0., 0.],\n",
       "          [0., 1., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0.]]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(out>0).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "55a0347f-93c0-4aaf-a136-94824a40a41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "class MaxPool2d:\n",
    "\n",
    "    def __init__(self, \n",
    "                 kernel_size: int|Tuple,\n",
    "                 stride: int|Tuple):\n",
    "        self.kernel_size = (kernel_size \n",
    "                            if isinstance(kernel_size, tuple) and len(kernel_size) == 2 \n",
    "                            else (kernel_size, kernel_size) \n",
    "                            if isinstance(kernel_size,int) else (2,2))\n",
    "        self.stride = (stride \n",
    "                            if isinstance(stride, tuple) and len(stride) == 2 \n",
    "                            else (stride, stride) \n",
    "                            if isinstance(stride,int) else (2,2))\n",
    "        self.kh , self.kw = self.kernel_size\n",
    "        self.sh, self.sw = self.stride\n",
    "\n",
    "    def prepare_submatrix(self, X: torch.Tensor):\n",
    "        B, C, ih, iw = X.shape\n",
    "        oh = (ih - self.kh) // self.sh + 1\n",
    "        ow = (iw - self.kw) // self.sw + 1\n",
    "        subM = X.unfold(2, self.kh, self.sh).unfold(3,self.kw, self.sw)\n",
    "        return subM\n",
    "\n",
    "    def forward(self, X:torch.Tensor):\n",
    "        subM = self.prepare_submatrix(X)\n",
    "        return subM.max(dim = -1).values.max(dim = -1).values\n",
    "\n",
    "    \n",
    "    def add_padding(self,\n",
    "                X: torch.Tensor,\n",
    "                padding: int):\n",
    "        padding = tuple(repeat(padding, 4))\n",
    "        batch_size, in_channels, original_height, original_width = X.size()\n",
    "        padded_height = original_height + padding[0] + padding[1]\n",
    "        padded_width = original_width + padding[2] + padding[3]\n",
    "    \n",
    "        padded_x = torch.zeros((batch_size, in_channels, padded_height, padded_width), dtype=X.dtype)\n",
    "        padded_x[:, :, padding[0]:padding[0] + original_height, padding[2]:padding[2] + original_width] = X\n",
    "        return padded_x\n",
    "\n",
    "    def prepare_mask(self,\n",
    "                     subm:torch.Tensor,\n",
    "                     kh:int,\n",
    "                     kw: int\n",
    "                    ):\n",
    "        B,C,oh,ow,kh,kw = subM.shape\n",
    "        a = subm.view(-1, kh * kw)\n",
    "        idx = torch.argmax(a ,dim=1)\n",
    "        b = torch.zeros_like(a)\n",
    "        b[torch.arange(b.shape[0]), idx] = 1\n",
    "        mask = b.view(B,C, oh,ow, kh,kw)\n",
    "        return mask\n",
    "\n",
    "    def mask_dXp(self, mask: torch.Tensor,\n",
    "                Xp: torch.Tensor,\n",
    "                dz: torch.Tensor,\n",
    "                kh: int,\n",
    "                kw: int):\n",
    "        dA = torch.einsum('i,ijk->ijk',\n",
    "                         dz.view(-1),\n",
    "                         mask.view(-1,kh,kw)).view(mask.shape)\n",
    "        B,C,ih,iw = Xp.shape\n",
    "        strides = (C * ih * iw, ih * iw, iw, 1)\n",
    "        strides = tuple(i * Xp.element_size() for i in strides)\n",
    "        dXp = torch.as_strided(dA, Xp.shape, strides)\n",
    "        return dXp\n",
    "        \n",
    "\n",
    "    def maxpool_backprop(self,\n",
    "                         dz:torch.Tensor,\n",
    "                         X: torch.Tensor\n",
    "                        ):\n",
    "        Xp = self.add_padding(X, self.kernel_size[0])\n",
    "        subM = self.prepare_submatrix(Xp)\n",
    "        B,C,oh,ow,kh,kw = subM.shape\n",
    "        B,C,ih,iw = Xp.shape\n",
    "\n",
    "        mask = self.prepare_mask(subm, kh,kw)\n",
    "        dXp = self.mask_dXp()\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c7bb8bc0-b1dc-448e-95ec-b5bade15a0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mxp = MaxPool2d(kernel_size=2,\n",
    "               stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d4bd85ee-b132-44a7-a44e-dc1b346f36f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mxp1 = mxp.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "327781cc-8b53-4c84-aba6-4676f45488b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.6551,  1.9962,  0.8629,  0.0646,  0.7071, -1.5948],\n",
       "          [-1.5375,  1.0167, -0.1964, -0.7298, -0.9535, -1.1809],\n",
       "          [-0.0379, -0.2264,  0.0209, -0.8308,  0.7298, -1.9761],\n",
       "          [ 0.5598, -0.8292, -0.8335,  0.6953,  1.0632,  1.2825],\n",
       "          [ 0.5851,  0.6493, -0.1973,  0.3548,  0.9868,  1.2589],\n",
       "          [-1.9253, -0.4256, -1.4989, -0.9786,  0.1233,  0.9412]],\n",
       "\n",
       "         [[-0.9156,  1.1602, -0.9697, -0.0564,  0.7820,  1.9830],\n",
       "          [ 1.0401, -1.0526, -1.1813,  2.8026,  0.4501, -1.2441],\n",
       "          [-2.3957, -1.2144,  0.9628, -1.1031, -0.7077,  0.7008],\n",
       "          [ 3.0552,  2.4094,  1.0904, -0.3627,  0.1904, -0.3447],\n",
       "          [ 0.2724, -1.4207,  0.1980,  0.9795,  0.7373, -0.1737],\n",
       "          [-0.6485,  0.7355, -1.5282,  0.0914,  0.1053, -2.8584]],\n",
       "\n",
       "         [[ 0.1064,  0.8753, -1.2745,  0.1193, -0.1274, -1.0164],\n",
       "          [ 1.8251, -2.0244, -0.9216,  1.3951,  0.9372,  0.4867],\n",
       "          [-0.2181,  0.0555, -0.7389,  1.7899, -0.0703,  0.5087],\n",
       "          [-0.9864, -1.2825, -0.7644, -0.0154, -0.4809,  0.2970],\n",
       "          [ 1.4239, -1.7034,  0.3287,  0.5828, -0.2428,  0.6769],\n",
       "          [ 0.1321,  0.6423,  0.5163, -1.6295,  0.8403, -1.6645]]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1,3,6,6)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "822ac2a2-90ba-4690-b060-3cd9686ac0a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.4049,  0.4919,  1.0858],\n",
       "          [ 0.9607,  1.5782,  1.3372],\n",
       "          [ 1.1338,  1.3063,  1.8908]],\n",
       "\n",
       "         [[ 1.5005,  0.7722,  0.6017],\n",
       "          [ 0.9588,  1.6036,  1.1714],\n",
       "          [ 1.2598,  1.0917,  1.8432]],\n",
       "\n",
       "         [[ 0.4777,  0.6339,  1.2369],\n",
       "          [ 0.9027,  0.0809,  1.9739],\n",
       "          [ 1.0262,  1.7149,  1.7282]]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.unfold(2,2,2).unfold(3,2,2).max(dim = -1).values.max(dim = -1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f01f4df-7db1-4032-bb62-351fd64eb25a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3, 3])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.unfold(2,2,2).unfold(3,2,2).max(dim = -1).values.max(dim = -1).values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f0de838-349a-48dc-9366-44e47066a9e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 6, got 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m a,b,c,d,e,f\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39munfold(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39munfold(\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmax(dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mmax(dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 6, got 4)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0838555-70cc-4a3a-ad6a-3731662dc402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[ 1.6551, -1.5375],\n",
       "           [ 1.9962,  1.0167],\n",
       "           [ 0.8629, -0.1964],\n",
       "           [ 0.0646, -0.7298],\n",
       "           [ 0.7071, -0.9535],\n",
       "           [-1.5948, -1.1809]],\n",
       "\n",
       "          [[-0.0379,  0.5598],\n",
       "           [-0.2264, -0.8292],\n",
       "           [ 0.0209, -0.8335],\n",
       "           [-0.8308,  0.6953],\n",
       "           [ 0.7298,  1.0632],\n",
       "           [-1.9761,  1.2825]],\n",
       "\n",
       "          [[ 0.5851, -1.9253],\n",
       "           [ 0.6493, -0.4256],\n",
       "           [-0.1973, -1.4989],\n",
       "           [ 0.3548, -0.9786],\n",
       "           [ 0.9868,  0.1233],\n",
       "           [ 1.2589,  0.9412]]],\n",
       "\n",
       "\n",
       "         [[[-0.9156,  1.0401],\n",
       "           [ 1.1602, -1.0526],\n",
       "           [-0.9697, -1.1813],\n",
       "           [-0.0564,  2.8026],\n",
       "           [ 0.7820,  0.4501],\n",
       "           [ 1.9830, -1.2441]],\n",
       "\n",
       "          [[-2.3957,  3.0552],\n",
       "           [-1.2144,  2.4094],\n",
       "           [ 0.9628,  1.0904],\n",
       "           [-1.1031, -0.3627],\n",
       "           [-0.7077,  0.1904],\n",
       "           [ 0.7008, -0.3447]],\n",
       "\n",
       "          [[ 0.2724, -0.6485],\n",
       "           [-1.4207,  0.7355],\n",
       "           [ 0.1980, -1.5282],\n",
       "           [ 0.9795,  0.0914],\n",
       "           [ 0.7373,  0.1053],\n",
       "           [-0.1737, -2.8584]]],\n",
       "\n",
       "\n",
       "         [[[ 0.1064,  1.8251],\n",
       "           [ 0.8753, -2.0244],\n",
       "           [-1.2745, -0.9216],\n",
       "           [ 0.1193,  1.3951],\n",
       "           [-0.1274,  0.9372],\n",
       "           [-1.0164,  0.4867]],\n",
       "\n",
       "          [[-0.2181, -0.9864],\n",
       "           [ 0.0555, -1.2825],\n",
       "           [-0.7389, -0.7644],\n",
       "           [ 1.7899, -0.0154],\n",
       "           [-0.0703, -0.4809],\n",
       "           [ 0.5087,  0.2970]],\n",
       "\n",
       "          [[ 1.4239,  0.1321],\n",
       "           [-1.7034,  0.6423],\n",
       "           [ 0.3287,  0.5163],\n",
       "           [ 0.5828, -1.6295],\n",
       "           [-0.2428,  0.8403],\n",
       "           [ 0.6769, -1.6645]]]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.unfold(2,2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46729fac-d7c3-48a0-a7ad-396fa355c197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[[ 1.6551,  1.9962],\n",
       "            [-1.5375,  1.0167]],\n",
       "\n",
       "           [[ 0.8629,  0.0646],\n",
       "            [-0.1964, -0.7298]],\n",
       "\n",
       "           [[ 0.7071, -1.5948],\n",
       "            [-0.9535, -1.1809]]],\n",
       "\n",
       "\n",
       "          [[[-0.0379, -0.2264],\n",
       "            [ 0.5598, -0.8292]],\n",
       "\n",
       "           [[ 0.0209, -0.8308],\n",
       "            [-0.8335,  0.6953]],\n",
       "\n",
       "           [[ 0.7298, -1.9761],\n",
       "            [ 1.0632,  1.2825]]],\n",
       "\n",
       "\n",
       "          [[[ 0.5851,  0.6493],\n",
       "            [-1.9253, -0.4256]],\n",
       "\n",
       "           [[-0.1973,  0.3548],\n",
       "            [-1.4989, -0.9786]],\n",
       "\n",
       "           [[ 0.9868,  1.2589],\n",
       "            [ 0.1233,  0.9412]]]],\n",
       "\n",
       "\n",
       "\n",
       "         [[[[-0.9156,  1.1602],\n",
       "            [ 1.0401, -1.0526]],\n",
       "\n",
       "           [[-0.9697, -0.0564],\n",
       "            [-1.1813,  2.8026]],\n",
       "\n",
       "           [[ 0.7820,  1.9830],\n",
       "            [ 0.4501, -1.2441]]],\n",
       "\n",
       "\n",
       "          [[[-2.3957, -1.2144],\n",
       "            [ 3.0552,  2.4094]],\n",
       "\n",
       "           [[ 0.9628, -1.1031],\n",
       "            [ 1.0904, -0.3627]],\n",
       "\n",
       "           [[-0.7077,  0.7008],\n",
       "            [ 0.1904, -0.3447]]],\n",
       "\n",
       "\n",
       "          [[[ 0.2724, -1.4207],\n",
       "            [-0.6485,  0.7355]],\n",
       "\n",
       "           [[ 0.1980,  0.9795],\n",
       "            [-1.5282,  0.0914]],\n",
       "\n",
       "           [[ 0.7373, -0.1737],\n",
       "            [ 0.1053, -2.8584]]]],\n",
       "\n",
       "\n",
       "\n",
       "         [[[[ 0.1064,  0.8753],\n",
       "            [ 1.8251, -2.0244]],\n",
       "\n",
       "           [[-1.2745,  0.1193],\n",
       "            [-0.9216,  1.3951]],\n",
       "\n",
       "           [[-0.1274, -1.0164],\n",
       "            [ 0.9372,  0.4867]]],\n",
       "\n",
       "\n",
       "          [[[-0.2181,  0.0555],\n",
       "            [-0.9864, -1.2825]],\n",
       "\n",
       "           [[-0.7389,  1.7899],\n",
       "            [-0.7644, -0.0154]],\n",
       "\n",
       "           [[-0.0703,  0.5087],\n",
       "            [-0.4809,  0.2970]]],\n",
       "\n",
       "\n",
       "          [[[ 1.4239, -1.7034],\n",
       "            [ 0.1321,  0.6423]],\n",
       "\n",
       "           [[ 0.3287,  0.5828],\n",
       "            [ 0.5163, -1.6295]],\n",
       "\n",
       "           [[-0.2428,  0.6769],\n",
       "            [ 0.8403, -1.6645]]]]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.unfold(2,2,2).unfold(3,2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a814ea59-491b-42e6-8cca-8ac0f91e5d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "mxp_ = x.unfold(2,2,2).unfold(3,2,2).max(dim = -1).values.max(dim = -1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "04d9c9d7-95a2-485e-9505-9c8bc8a9a3a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1],\n",
      "        [2, 3],\n",
      "        [4, 5],\n",
      "        [6, 7],\n",
      "        [8, 9]]) \n",
      "\n",
      "tensor([[0, 1],\n",
      "        [1, 2],\n",
      "        [2, 3],\n",
      "        [3, 4],\n",
      "        [4, 5],\n",
      "        [5, 6],\n",
      "        [6, 7],\n",
      "        [7, 8],\n",
      "        [8, 9]])\n",
      "tensor([[0, 1],\n",
      "        [3, 4],\n",
      "        [6, 7]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.arange(0,10).unfold(0,2,2),'\\n')\n",
    "print(torch.arange(0,10).unfold(0,2,1))\n",
    "print(torch.arange(0,10).unfold(0,2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "52d8d9c2-9b1b-45a1-a7e5-a0025fccad98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(mxp1 == mxp_).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "93f1159d-9d53-4d61-85a4-7003d794693e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape after convolution torch.Size([10, 8, 30, 30]) \n",
      "shape after relu torch.Size([10, 8, 30, 30]) \n",
      "shape after max pool torch.Size([10, 8, 15, 15]) \n",
      "shape after fc layer torch.Size([10, 128]) \n",
      "shape after relu 2 torch.Size([10, 128]) \n",
      "shape after FCL output torch.Size([10, 3]) \n",
      "Loss: 1.094382643699646\n",
      "shape of self.last_ip torch.Size([10, 128]) , shape of self.d_L_d_out torch.Size([10, 3])shape of self.weights torch.Size([128, 3])\n",
      "shape of d_L_d_weights torch.Size([128, 3])\n",
      "shape of self.last_ip torch.Size([10, 1800]) , shape of self.d_L_d_out torch.Size([10, 128])shape of self.weights torch.Size([1800, 128])\n",
      "shape of d_L_d_weights torch.Size([1800, 128])\n",
      "inside relu derivative  torch.Size([10, 1800])\n",
      "torch.Size([10, 1800])\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "einsum(): subscript i has size 23120 for operand 1 which does not broadcast with previously seen size 18000",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 281\u001b[0m\n\u001b[0;32m    279\u001b[0m cnn \u001b[38;5;241m=\u001b[39m CNN()\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):  \u001b[38;5;66;03m# Reduced epochs for example purposes\u001b[39;00m\n\u001b[1;32m--> 281\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mcnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[54], line 267\u001b[0m, in \u001b[0;36mCNN.train\u001b[1;34m(self, x_train, y_train, lr)\u001b[0m\n\u001b[0;32m    265\u001b[0m grad \u001b[38;5;241m=\u001b[39m relu_derivative(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc\u001b[38;5;241m.\u001b[39mbackward(grad, lr))\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28mprint\u001b[39m(grad\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m--> 267\u001b[0m grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaxpool_backprop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    268\u001b[0m grad \u001b[38;5;241m=\u001b[39m relu_derivative(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv\u001b[38;5;241m.\u001b[39mbackward(grad, lr))\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "Cell \u001b[1;32mIn[54], line 201\u001b[0m, in \u001b[0;36mMaxPool2d.maxpool_backprop\u001b[1;34m(self, dz)\u001b[0m\n\u001b[0;32m    198\u001b[0m B,C,ih,iw \u001b[38;5;241m=\u001b[39m Xp\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    200\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_mask(subM, kh,kw)\n\u001b[1;32m--> 201\u001b[0m dXp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmask_dXp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkh\u001b[49m\u001b[43m,\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dXp\n",
      "Cell \u001b[1;32mIn[54], line 181\u001b[0m, in \u001b[0;36mMaxPool2d.mask_dXp\u001b[1;34m(self, mask, Xp, dz, kh, kw)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmask_dXp\u001b[39m(\u001b[38;5;28mself\u001b[39m, mask: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m    177\u001b[0m             Xp: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m    178\u001b[0m             dz: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m    179\u001b[0m             kh: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m    180\u001b[0m             kw: \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m--> 181\u001b[0m     dA \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mi,ijk->ijk\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mdz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mkh\u001b[49m\u001b[43m,\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(mask\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    184\u001b[0m     B,C,ih,iw \u001b[38;5;241m=\u001b[39m Xp\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    185\u001b[0m     strides \u001b[38;5;241m=\u001b[39m (C \u001b[38;5;241m*\u001b[39m ih \u001b[38;5;241m*\u001b[39m iw, ih \u001b[38;5;241m*\u001b[39m iw, iw, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\functional.py:385\u001b[0m, in \u001b[0;36meinsum\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[1;32m--> 385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    387\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[1;31mRuntimeError\u001b[0m: einsum(): subscript i has size 23120 for operand 1 which does not broadcast with previously seen size 18000"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from itertools import repeat\n",
    "\n",
    "\n",
    "# Helper functions\n",
    "def relu(x):\n",
    "    return torch.clamp(x, min=0)\n",
    "\n",
    "\n",
    "def relu_derivative(x):\n",
    "    print(f\"inside relu derivative  {x.shape}\")\n",
    "    return (x > 0).float()\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    exps = torch.exp(x - torch.max(x, dim=1, keepdim=True)[0])\n",
    "    return exps / torch.sum(exps, dim=1, keepdim=True)\n",
    "\n",
    "\n",
    "def cross_entropy_loss(y_pred, y_true):\n",
    "    n_samples = y_pred.shape[0]\n",
    "    log_likelihood = -torch.log(y_pred[range(n_samples), y_true])\n",
    "    return torch.sum(log_likelihood) / n_samples\n",
    "\n",
    "\n",
    "def cross_entropy_loss_derivative(y_pred, y_true):\n",
    "    n_samples = y_pred.shape[0]\n",
    "    # grad = y_pred.clone()\n",
    "    # grad[range(n_samples), y_true] -= 1\n",
    "    # grad = grad / n_samples\n",
    "    grad = F.softmax(y_pred, 1)\n",
    "    grad[range(n_samples), y_true] -= 1\n",
    "    grad = grad / n_samples\n",
    "    return grad\n",
    "\n",
    "\n",
    "# Custom Conv2d Layer\n",
    "class Conv2d:\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 kernel_size: int,\n",
    "                 stride: int = 1,\n",
    "                 padding: int = 0,\n",
    "                 dilation: int = 1,\n",
    "                 groups: int = 1):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.groups = groups\n",
    "        self._check_parameters()\n",
    "        self._n_tuple()\n",
    "        self.weights, self.bias = self.initialise_weights()\n",
    "\n",
    "    def _n_tuple(self):\n",
    "        self.kernel_size = (self.kernel_size, self.kernel_size)\n",
    "        self.stride = (self.stride, self.stride)\n",
    "        self.padding = (self.padding, self.padding)\n",
    "        self.dilation = (self.dilation, self.dilation)\n",
    "\n",
    "    def initialise_weights(self):\n",
    "        return (torch.randn(self.out_channels, self.in_channels // self.groups, *self.kernel_size, requires_grad=True),\n",
    "                torch.zeros(self.out_channels, requires_grad=True))\n",
    "\n",
    "    def add_padding(self, x: torch.Tensor, padding: int):\n",
    "        padding = tuple(repeat(padding, 4))\n",
    "        batch_size, in_channels, original_height, original_width = x.size()\n",
    "        padded_height = original_height + padding[0] + padding[1]\n",
    "        padded_width = original_width + padding[2] + padding[3]\n",
    "\n",
    "        padded_x = torch.zeros((batch_size, in_channels, padded_height, padded_width), dtype=x.dtype)\n",
    "        padded_x[:, :, padding[0]:padding[0] + original_height, padding[2]:padding[2] + original_width] = x\n",
    "        return padded_x\n",
    "\n",
    "    def _check_parameters(self):\n",
    "        if self.groups <= 0:\n",
    "            raise ValueError('groups must be a positive integer')\n",
    "        if self.in_channels % self.groups != 0:\n",
    "            raise ValueError('in_channels should be divisible by groups')\n",
    "        if self.out_channels % self.groups != 0:\n",
    "            raise ValueError('out_channels should be divisible by groups')\n",
    "\n",
    "    def __call__(self, x: torch.Tensor):\n",
    "        batch_size, in_channels, in_height, in_width = x.size()\n",
    "        out_height = (in_height + 2 * self.padding[0] - self.dilation[0] * (self.kernel_size[0] - 1) - 1) // \\\n",
    "                     self.stride[0] + 1\n",
    "        out_width = (in_width + 2 * self.padding[1] - self.dilation[1] * (self.kernel_size[1] - 1) - 1) // self.stride[\n",
    "            1] + 1\n",
    "\n",
    "        if self.padding[0] > 0 or self.padding[1] > 0:\n",
    "            x = self.add_padding(x, self.padding[0])\n",
    "\n",
    "        out = torch.zeros(batch_size, self.out_channels, out_height, out_width)\n",
    "\n",
    "        for h in range(out_height):\n",
    "            for w in range(out_width):\n",
    "                h_start = h * self.stride[0]\n",
    "                h_end = h_start + self.kernel_size[0]\n",
    "                w_start = w * self.stride[1]\n",
    "                w_end = w_start + self.kernel_size[1]\n",
    "                receptive_field = x[:, :, h_start:h_end, w_start:w_end]\n",
    "\n",
    "                out[:, :, h, w] = torch.sum(\n",
    "                    receptive_field.unsqueeze(1) * self.weights.view(1, self.out_channels,\n",
    "                                                                     self.in_channels // self.groups,\n",
    "                                                                     *self.kernel_size),\n",
    "                    dim=(2, 3, 4)\n",
    "                ) + self.bias.view(1, self.out_channels)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "# Max Pool Layer\n",
    "from typing import Tuple\n",
    "\n",
    "class MaxPool2d:\n",
    "\n",
    "    def __init__(self, \n",
    "                 kernel_size: int|Tuple,\n",
    "                 stride: int|Tuple):\n",
    "        self.kernel_size = (kernel_size \n",
    "                            if isinstance(kernel_size, tuple) and len(kernel_size) == 2 \n",
    "                            else (kernel_size, kernel_size) \n",
    "                            if isinstance(kernel_size,int) else (2,2))\n",
    "        self.stride = (stride \n",
    "                            if isinstance(stride, tuple) and len(stride) == 2 \n",
    "                            else (stride, stride) \n",
    "                            if isinstance(stride,int) else (2,2))\n",
    "        self.kh , self.kw = self.kernel_size\n",
    "        self.sh, self.sw = self.stride\n",
    "\n",
    "    def prepare_submatrix(self, X: torch.Tensor):\n",
    "        B, C, ih, iw = X.shape\n",
    "        oh = (ih - self.kh) // self.sh + 1\n",
    "        ow = (iw - self.kw) // self.sw + 1\n",
    "        subM = X.unfold(2, self.kh, self.sh).unfold(3,self.kw, self.sw)\n",
    "        return subM\n",
    "\n",
    "    def forward(self, X:torch.Tensor):\n",
    "        self.X = X\n",
    "        subM = self.prepare_submatrix(X)\n",
    "        return subM.max(dim = -1).values.max(dim = -1).values\n",
    "\n",
    "    \n",
    "    def add_padding(self,\n",
    "                X: torch.Tensor,\n",
    "                padding: int):\n",
    "        padding = tuple(repeat(padding, 4))\n",
    "        batch_size, in_channels, original_height, original_width = X.size()\n",
    "        padded_height = original_height + padding[0] + padding[1]\n",
    "        padded_width = original_width + padding[2] + padding[3]\n",
    "    \n",
    "        padded_x = torch.zeros((batch_size, in_channels, padded_height, padded_width), dtype=X.dtype)\n",
    "        padded_x[:, :, padding[0]:padding[0] + original_height, padding[2]:padding[2] + original_width] = X\n",
    "        return padded_x\n",
    "\n",
    "    def prepare_mask(self,\n",
    "                     subM:torch.Tensor,\n",
    "                     kh:int,\n",
    "                     kw: int\n",
    "                    ):\n",
    "        B,C,oh,ow,kh,kw = subM.shape\n",
    "        print(torch.reshape(subM,(-1, kh * kw)))\n",
    "        # a = subM.view(-1, kh * kw)\n",
    "        a = torch.reshape(subM,(-1, kh * kw))\n",
    "        idx = torch.argmax(a ,dim=1)\n",
    "        b = torch.zeros_like(a)\n",
    "        b[torch.arange(b.shape[0]), idx] = 1\n",
    "        mask = b.view(B,C, oh,ow, kh,kw)\n",
    "        return mask\n",
    "\n",
    "    def mask_dXp(self, mask: torch.Tensor,\n",
    "                Xp: torch.Tensor,\n",
    "                dz: torch.Tensor,\n",
    "                kh: int,\n",
    "                kw: int):\n",
    "        dA = torch.einsum('i,ijk->ijk',\n",
    "                         dz.view(-1),\n",
    "                         mask.view(-1,kh,kw)).view(mask.shape)\n",
    "        B,C,ih,iw = Xp.shape\n",
    "        strides = (C * ih * iw, ih * iw, iw, 1)\n",
    "        strides = tuple(i * Xp.element_size() for i in strides)\n",
    "        dXp = torch.as_strided(dA, Xp.shape, strides)\n",
    "        return dXp\n",
    "        \n",
    "\n",
    "    def maxpool_backprop(self,\n",
    "                         dz:torch.Tensor,\n",
    "                        ):\n",
    "        X = self.X\n",
    "        Xp = self.add_padding(X, self.kernel_size[0])\n",
    "        subM = self.prepare_submatrix(Xp)\n",
    "        B,C,oh,ow,kh,kw = subM.shape\n",
    "        B,C,ih,iw = Xp.shape\n",
    "\n",
    "        mask = self.prepare_mask(subM, kh,kw)\n",
    "        dXp = self.mask_dXp(mask, Xp, dz, kh,kw)\n",
    "        return dXp\n",
    "        \n",
    "                \n",
    "\n",
    "\n",
    "# Fully Connected Layer\n",
    "class FCLayer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = torch.randn(input_size, output_size, requires_grad=True) / input_size\n",
    "        self.biases = torch.zeros(output_size, requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.last_input_shape = x.shape\n",
    "        self.last_input = x.view(x.shape[0], -1)\n",
    "        # return torch.matmul(self.last_input, self.weights) + self.biases\n",
    "        return (self.last_input @ self.weights) + self.biases\n",
    "    def backward(self, d_L_d_out, lr):\n",
    "        # d_L_d_weights = torch.matmul(self.last_input.t(), d_L_d_out)\n",
    "        print(f\"shape of self.last_ip {self.last_input.shape} , shape of self.d_L_d_out {d_L_d_out.shape}\"\n",
    "              f\"shape of self.weights {self.weights.shape}\")\n",
    "        d_L_d_weights = self.last_input.T @ d_L_d_out\n",
    "        print(f\"shape of d_L_d_weights {d_L_d_weights.shape}\")\n",
    "        d_L_d_biases = torch.sum(d_L_d_out, dim=0)\n",
    "\n",
    "        d_L_d_input = d_L_d_out @ self.weights.T\n",
    "        self.weights = self.weights - lr * d_L_d_weights\n",
    "        self.biases = self.biases - lr * d_L_d_biases\n",
    "\n",
    "        return d_L_d_input\n",
    "\n",
    "\n",
    "# Model\n",
    "class CNN:\n",
    "    def __init__(self):\n",
    "        self.conv = Conv2d(3, 8, 3)\n",
    "        self.pool = MaxPool2d(2,2)\n",
    "        self.fc = FCLayer(8 * 15 * 15, 128)\n",
    "        self.output = FCLayer(128, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        print(f\"shape after convolution {x.shape} \")\n",
    "        x = relu(x)\n",
    "        print(f\"shape after relu {x.shape} \")\n",
    "        x = self.pool.forward(x)\n",
    "        print(f\"shape after max pool {x.shape} \")\n",
    "        x = self.fc.forward(x)\n",
    "        print(f\"shape after fc layer {x.shape} \")\n",
    "        x = relu(x)\n",
    "        print(f\"shape after relu 2 {x.shape} \")\n",
    "        x = self.output.forward(x)\n",
    "        print(f\"shape after FCL output {x.shape} \")\n",
    "        return softmax(x)\n",
    "\n",
    "    def train(self, x_train, y_train, lr=0.001):\n",
    "        out = self.forward(x_train)\n",
    "        loss = cross_entropy_loss(out, y_train)\n",
    "        print(f'Loss: {loss.item()}')\n",
    "\n",
    "        grad = cross_entropy_loss_derivative(out, y_train)\n",
    "        # print(grad.shape)\n",
    "        grad = self.output.backward(grad, lr)\n",
    "        # print(grad.shape)\n",
    "        grad = relu_derivative(self.fc.backward(grad, lr))\n",
    "        print(grad.shape)\n",
    "        grad = self.pool.maxpool_backprop(grad)\n",
    "        grad = relu_derivative(self.conv.backward(grad, lr))\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "# Create random x_train and y_train\n",
    "torch.manual_seed(42)\n",
    "x_train = torch.randn(10, 3, 32, 32)  # 10 samples, 3 channels, 32x32 images\n",
    "y_train = torch.randint(0, 3, (10,))  # 10 samples, 3 classes\n",
    "\n",
    "# Example usage\n",
    "cnn = CNN()\n",
    "for epoch in range(10):  # Reduced epochs for example purposes\n",
    "    loss = cnn.train(x_train, y_train)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "58a9eac8-bca2-4e1e-be54-29b0c9d5c992",
   "metadata": {},
   "outputs": [],
   "source": [
    "a =torch.randn(10, 8, 17, 17, 2, 2)\n",
    "b = a.view(-1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "447eae02-ae9e-4b17-a362-b397dba22e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from itertools import repeat\n",
    "\n",
    "\n",
    "# Helper functions\n",
    "def relu(x):\n",
    "    return torch.clamp(x, min=0)\n",
    "\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).float()\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    exps = torch.exp(x - torch.max(x, dim=1, keepdim=True)[0])\n",
    "    return exps / torch.sum(exps, dim=1, keepdim=True)\n",
    "\n",
    "\n",
    "def cross_entropy_loss(y_pred, y_true):\n",
    "    n_samples = y_pred.shape[0]\n",
    "    log_likelihood = -torch.log(y_pred[range(n_samples), y_true])\n",
    "    return torch.sum(log_likelihood) / n_samples\n",
    "\n",
    "\n",
    "def cross_entropy_loss_derivative(y_pred, y_true):\n",
    "    n_samples = y_pred.shape[0]\n",
    "    grad = F.softmax(y_pred, 1)\n",
    "    grad[range(n_samples), y_true] -= 1\n",
    "    grad = grad / n_samples\n",
    "    return grad\n",
    "\n",
    "\n",
    "# Custom Conv2d Layer\n",
    "class Conv2d:\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 kernel_size: int,\n",
    "                 stride: int = 1,\n",
    "                 padding: int = 0,\n",
    "                 dilation: int = 1,\n",
    "                 groups: int = 1):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.groups = groups\n",
    "        self._check_parameters()\n",
    "        self._n_tuple()\n",
    "        self.weights, self.bias = self.initialise_weights()\n",
    "\n",
    "    def _n_tuple(self):\n",
    "        self.kernel_size = (self.kernel_size, self.kernel_size)\n",
    "        self.stride = (self.stride, self.stride)\n",
    "        self.padding = (self.padding, self.padding)\n",
    "        self.dilation = (self.dilation, self.dilation)\n",
    "\n",
    "    def initialise_weights(self):\n",
    "        return (torch.randn(self.out_channels, self.in_channels // self.groups, *self.kernel_size, requires_grad=True),\n",
    "                torch.zeros(self.out_channels, requires_grad=True))\n",
    "\n",
    "    def add_padding(self, x: torch.Tensor, padding: int):\n",
    "        padding = tuple(repeat(padding, 4))\n",
    "        batch_size, in_channels, original_height, original_width = x.size()\n",
    "        padded_height = original_height + padding[0] + padding[1]\n",
    "        padded_width = original_width + padding[2] + padding[3]\n",
    "\n",
    "        padded_x = torch.zeros((batch_size, in_channels, padded_height, padded_width), dtype=x.dtype)\n",
    "        padded_x[:, :, padding[0]:padding[0] + original_height, padding[2]:padding[2] + original_width] = x\n",
    "        return padded_x\n",
    "\n",
    "    def _check_parameters(self):\n",
    "        if self.groups <= 0:\n",
    "            raise ValueError('groups must be a positive integer')\n",
    "        if self.in_channels % self.groups != 0:\n",
    "            raise ValueError('in_channels should be divisible by groups')\n",
    "        if self.out_channels % self.groups != 0:\n",
    "            raise ValueError('out_channels should be divisible by groups')\n",
    "\n",
    "    def __call__(self, x: torch.Tensor):\n",
    "        batch_size, in_channels, in_height, in_width = x.size()\n",
    "        out_height = (in_height + 2 * self.padding[0] - self.dilation[0] * (self.kernel_size[0] - 1) - 1) // \\\n",
    "                     self.stride[0] + 1\n",
    "        out_width = (in_width + 2 * self.padding[1] - self.dilation[1] * (self.kernel_size[1] - 1) - 1) // self.stride[\n",
    "            1] + 1\n",
    "\n",
    "        if self.padding[0] > 0 or self.padding[1] > 0:\n",
    "            x = self.add_padding(x, self.padding[0])\n",
    "\n",
    "        out = torch.zeros(batch_size, self.out_channels, out_height, out_width)\n",
    "\n",
    "        for h in range(out_height):\n",
    "            for w in range(out_width):\n",
    "                h_start = h * self.stride[0]\n",
    "                h_end = h_start + self.kernel_size[0]\n",
    "                w_start = w * self.stride[1]\n",
    "                w_end = w_start + self.kernel_size[1]\n",
    "                receptive_field = x[:, :, h_start:h_end, w_start:w_end]\n",
    "\n",
    "                out[:, :, h, w] = torch.sum(\n",
    "                    receptive_field.unsqueeze(1) * self.weights.view(1, self.out_channels,\n",
    "                                                                     self.in_channels // self.groups,\n",
    "                                                                     *self.kernel_size),\n",
    "                    dim=(2, 3, 4)\n",
    "                ) + self.bias.view(1, self.out_channels)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "# Max Pool Layer\n",
    "from typing import Tuple\n",
    "\n",
    "class MaxPool2d:\n",
    "    def __init__(self, \n",
    "                 kernel_size: int|Tuple,\n",
    "                 stride: int|Tuple):\n",
    "        self.kernel_size = (kernel_size \n",
    "                            if isinstance(kernel_size, tuple) and len(kernel_size) == 2 \n",
    "                            else (kernel_size, kernel_size) \n",
    "                            if isinstance(kernel_size,int) else (2,2))\n",
    "        self.stride = (stride \n",
    "                            if isinstance(stride, tuple) and len(stride) == 2 \n",
    "                            else (stride, stride) \n",
    "                            if isinstance(stride,int) else (2,2))\n",
    "        self.kh , self.kw = self.kernel_size\n",
    "        self.sh, self.sw = self.stride\n",
    "\n",
    "    def prepare_submatrix(self, X: torch.Tensor):\n",
    "        B, C, ih, iw = X.shape\n",
    "        oh = (ih - self.kh) // self.sh + 1\n",
    "        ow = (iw - self.kw) // self.sw + 1\n",
    "        subM = X.unfold(2, self.kh, self.sh).unfold(3, self.kw, self.sw)\n",
    "        return subM\n",
    "\n",
    "    def forward(self, X: torch.Tensor):\n",
    "        self.X = X\n",
    "        subM = self.prepare_submatrix(X)\n",
    "        return subM.max(dim=-1).values.max(dim=-1).values\n",
    "\n",
    "    def add_padding(self, X: torch.Tensor, padding: int):\n",
    "        padding = tuple(repeat(padding, 4))\n",
    "        batch_size, in_channels, original_height, original_width = X.size()\n",
    "        padded_height = original_height + padding[0] + padding[1]\n",
    "        padded_width = original_width + padding[2] + padding[3]\n",
    "\n",
    "        padded_x = torch.zeros((batch_size, in_channels, padded_height, padded_width), dtype=X.dtype)\n",
    "        padded_x[:, :, padding[0]:padding[0] + original_height, padding[2]:padding[2] + original_width] = X\n",
    "        return padded_x\n",
    "\n",
    "    def prepare_mask(self, subM: torch.Tensor, kh: int, kw: int):\n",
    "        B, C, oh, ow, kh, kw = subM.shape\n",
    "        a = torch.reshape(subM, (-1, kh * kw))\n",
    "        idx = torch.argmax(a, dim=1)\n",
    "        b = torch.zeros_like(a)\n",
    "        b[torch.arange(b.shape[0]), idx] = 1\n",
    "        mask = b.view(B, C, oh, ow, kh, kw)\n",
    "        return mask\n",
    "\n",
    "    def mask_dXp(self, mask: torch.Tensor, Xp: torch.Tensor, dz: torch.Tensor, kh: int, kw: int):\n",
    "        dA = torch.einsum('i,ijk->ijk', dz.view(-1), mask.view(-1, kh, kw)).view(mask.shape)\n",
    "        B, C, ih, iw = Xp.shape\n",
    "        strides = (C * ih * iw, ih * iw, iw, 1)\n",
    "        strides = tuple(i * Xp.element_size() for i in strides)\n",
    "        dXp = torch.as_strided(dA, Xp.shape, strides)\n",
    "        return dXp\n",
    "\n",
    "    def maxpool_backprop(self, dz: torch.Tensor):\n",
    "        X = self.X\n",
    "        Xp = self.add_padding(X, self.kernel_size[0])\n",
    "        subM = self.prepare_submatrix(Xp)\n",
    "        B, C, oh, ow, kh, kw = subM.shape\n",
    "        B, C, ih, iw = Xp.shape\n",
    "\n",
    "        mask = self.prepare_mask(subM, kh, kw)\n",
    "        dXp = self.mask_dXp(mask, Xp, dz, kh, kw)\n",
    "        return dXp\n",
    "\n",
    "\n",
    "# Fully Connected Layer\n",
    "class FCLayer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = torch.randn(input_size, output_size, requires_grad=True) / input_size\n",
    "        self.biases = torch.zeros(output_size, requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.last_input_shape = x.shape\n",
    "        self.last_input = x.view(x.shape[0], -1)\n",
    "        return (self.last_input @ self.weights) + self.biases\n",
    "\n",
    "    def backward(self, d_L_d_out, lr):\n",
    "        d_L_d_weights = self.last_input.T @ d_L_d_out\n",
    "        d_L_d_biases = d_L_d_out.sum(axis=0)\n",
    "        d_L_d_input = d_L_d_out @ self.weights.T\n",
    "        self.weights.grad = d_L_d_weights\n",
    "        self.biases.grad = d_L_d_biases\n",
    "        self.weights = self.weights - lr * d_L_d_weights\n",
    "        self.biases = self.biases - lr * d_L_d_biases\n",
    "        return d_L_d_input.view(self.last_input_shape)\n",
    "\n",
    "\n",
    "# Model definition\n",
    "class SimpleCNN:\n",
    "    def __init__(self):\n",
    "        self.conv1 = Conv2d(1, 8, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool1 = MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = FCLayer(14 * 14 * 8, 128)\n",
    "        self.fc2 = FCLayer(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = relu(self.conv1(x))\n",
    "        x = self.pool1.forward(x)\n",
    "        x = relu(self.fc1.forward(x))\n",
    "        x = self.fc2.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, dL_dy, lr):\n",
    "        dL_dy = self.fc2.backward(dL_dy, lr)\n",
    "        dL_dy = self.fc1.backward(dL_dy, lr)\n",
    "        dL_dy = self.pool1.maxpool_backprop(dL_dy)\n",
    "        dL_dy = dL_dy * relu_derivative(self.conv1.forward(self.pool1.X))\n",
    "        self.conv1.weights.grad = torch.einsum('ijkl,ijmn->jklm', dL_dy, self.pool1.X)\n",
    "        self.conv1.bias.grad = dL_dy.sum(dim=(0, 2, 3))\n",
    "        self.conv1.weights -= lr * self.conv1.weights.grad\n",
    "        self.conv1.bias -= lr * self.conv1.bias.grad\n",
    "\n",
    "\n",
    "# Training loop (example)\n",
    "def train(model, criterion, optimizer, train_loader, num_epochs=5):\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            # Forward pass\n",
    "            outputs = model.forward(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            dL_dy = cross_entropy_loss_derivative(outputs, labels)\n",
    "            model.backward(dL_dy, lr=0.001)\n",
    "\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "torch.manual_seed(42)\n",
    "x_train = torch.randn(10, 3, 32, 32)  # 10 samples, 3 channels, 32x32 images\n",
    "y_train = torch.randint(0, 3, (10,))  # 10 samples, 3 classes\n",
    "\n",
    "# Example usage\n",
    "cnn = CNN()\n",
    "for epoch in range(10):  # Reduced epochs for example purposes\n",
    "    loss = cnn.train(x_train, y_train)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "63677239-71cc-495d-9ad4-065bb259feea",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "einsum(): subscript i has size 65536 for operand 1 which does not broadcast with previously seen size 50176",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m get_dataloader()\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[55], line 240\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, criterion, optimizer, train_loader, num_epochs)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[0;32m    239\u001b[0m dL_dy \u001b[38;5;241m=\u001b[39m cross_entropy_loss_derivative(outputs, labels)\n\u001b[1;32m--> 240\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdL_dy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Step [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[55], line 222\u001b[0m, in \u001b[0;36mSimpleCNN.backward\u001b[1;34m(self, dL_dy, lr)\u001b[0m\n\u001b[0;32m    220\u001b[0m dL_dy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2\u001b[38;5;241m.\u001b[39mbackward(dL_dy, lr)\n\u001b[0;32m    221\u001b[0m dL_dy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1\u001b[38;5;241m.\u001b[39mbackward(dL_dy, lr)\n\u001b[1;32m--> 222\u001b[0m dL_dy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpool1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaxpool_backprop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdL_dy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    223\u001b[0m dL_dy \u001b[38;5;241m=\u001b[39m dL_dy \u001b[38;5;241m*\u001b[39m relu_derivative(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool1\u001b[38;5;241m.\u001b[39mX))\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1\u001b[38;5;241m.\u001b[39mweights\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mijkl,ijmn->jklm\u001b[39m\u001b[38;5;124m'\u001b[39m, dL_dy, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool1\u001b[38;5;241m.\u001b[39mX)\n",
      "Cell \u001b[1;32mIn[55], line 178\u001b[0m, in \u001b[0;36mMaxPool2d.maxpool_backprop\u001b[1;34m(self, dz)\u001b[0m\n\u001b[0;32m    175\u001b[0m B, C, ih, iw \u001b[38;5;241m=\u001b[39m Xp\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    177\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_mask(subM, kh, kw)\n\u001b[1;32m--> 178\u001b[0m dXp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmask_dXp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dXp\n",
      "Cell \u001b[1;32mIn[55], line 163\u001b[0m, in \u001b[0;36mMaxPool2d.mask_dXp\u001b[1;34m(self, mask, Xp, dz, kh, kw)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmask_dXp\u001b[39m(\u001b[38;5;28mself\u001b[39m, mask: torch\u001b[38;5;241m.\u001b[39mTensor, Xp: torch\u001b[38;5;241m.\u001b[39mTensor, dz: torch\u001b[38;5;241m.\u001b[39mTensor, kh: \u001b[38;5;28mint\u001b[39m, kw: \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m--> 163\u001b[0m     dA \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mi,ijk->ijk\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(mask\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    164\u001b[0m     B, C, ih, iw \u001b[38;5;241m=\u001b[39m Xp\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    165\u001b[0m     strides \u001b[38;5;241m=\u001b[39m (C \u001b[38;5;241m*\u001b[39m ih \u001b[38;5;241m*\u001b[39m iw, ih \u001b[38;5;241m*\u001b[39m iw, iw, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\functional.py:385\u001b[0m, in \u001b[0;36meinsum\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[1;32m--> 385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    387\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[1;31mRuntimeError\u001b[0m: einsum(): subscript i has size 65536 for operand 1 which does not broadcast with previously seen size 50176"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Create synthetic dataset\n",
    "def create_synthetic_data(num_samples=1000):\n",
    "    # Generate random images of size 28x28 with 1 channel (grayscale)\n",
    "    images = torch.randn(num_samples, 1, 28, 28)\n",
    "    # Generate random labels (10 classes)\n",
    "    labels = torch.randint(0, 10, (num_samples,))\n",
    "    return images, labels\n",
    "\n",
    "# Create DataLoader\n",
    "def get_dataloader(batch_size=32, num_samples=1000):\n",
    "    images, labels = create_synthetic_data(num_samples)\n",
    "    dataset = TensorDataset(images, labels)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return dataloader\n",
    "\n",
    "# Instantiate the model\n",
    "model = SimpleCNN()\n",
    "\n",
    "# Define the criterion\n",
    "criterion = cross_entropy_loss\n",
    "\n",
    "# Get the DataLoader\n",
    "train_loader = get_dataloader()\n",
    "\n",
    "# Training loop\n",
    "train(model, criterion, None, train_loader, num_epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3a738c42-409c-4ac8-a370-c2f251188bcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Batch 1/32, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CD138JR\\AppData\\Local\\Temp\\ipykernel_39268\\3658936679.py:260: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\CD138JR\\AppData\\Local\\Temp\\ipykernel_39268\\3658936679.py:266: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  if param.grad is not None:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Batch 2/32, Loss: nan\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 301\u001b[0m\n\u001b[0;32m    298\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m get_dataloader()\n\u001b[0;32m    300\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[1;32m--> 301\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[57], line 248\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, criterion, optimizer, dataloader, num_epochs)\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataloader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    247\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 248\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    249\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from itertools import repeat\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "# Helper functions\n",
    "def relu(x):\n",
    "    return torch.clamp(x, min=0)\n",
    "\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).float()\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    exps = torch.exp(x - torch.max(x, dim=1, keepdim=True)[0])\n",
    "    return exps / torch.sum(exps, dim=1, keepdim=True)\n",
    "\n",
    "\n",
    "def cross_entropy_loss(y_pred, y_true):\n",
    "    n_samples = y_pred.shape[0]\n",
    "    log_likelihood = -torch.log(y_pred[range(n_samples), y_true])\n",
    "    return torch.sum(log_likelihood) / n_samples\n",
    "\n",
    "\n",
    "def cross_entropy_loss_derivative(y_pred, y_true):\n",
    "    n_samples = y_pred.shape[0]\n",
    "    grad = F.softmax(y_pred, dim=1)\n",
    "    grad[range(n_samples), y_true] -= 1\n",
    "    grad = grad / n_samples\n",
    "    return grad\n",
    "\n",
    "\n",
    "# Custom Conv2d Layer\n",
    "class Conv2d:\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 kernel_size: int,\n",
    "                 stride: int = 1,\n",
    "                 padding: int = 0,\n",
    "                 dilation: int = 1,\n",
    "                 groups: int = 1):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.groups = groups\n",
    "        self._check_parameters()\n",
    "        self._n_tuple()\n",
    "        self.weights, self.bias = self.initialise_weights()\n",
    "\n",
    "    def _n_tuple(self):\n",
    "        self.kernel_size = (self.kernel_size, self.kernel_size)\n",
    "        self.stride = (self.stride, self.stride)\n",
    "        self.padding = (self.padding, self.padding)\n",
    "        self.dilation = (self.dilation, self.dilation)\n",
    "\n",
    "    def initialise_weights(self):\n",
    "        return (torch.randn(self.out_channels, self.in_channels // self.groups, *self.kernel_size, requires_grad=True),\n",
    "                torch.zeros(self.out_channels, requires_grad=True))\n",
    "\n",
    "    def add_padding(self, x: torch.Tensor, padding: int):\n",
    "        padding = tuple(repeat(padding, 4))\n",
    "        batch_size, in_channels, original_height, original_width = x.size()\n",
    "        padded_height = original_height + padding[0] + padding[1]\n",
    "        padded_width = original_width + padding[2] + padding[3]\n",
    "\n",
    "        padded_x = torch.zeros((batch_size, in_channels, padded_height, padded_width), dtype=x.dtype)\n",
    "        padded_x[:, :, padding[0]:padding[0] + original_height, padding[2]:padding[2] + original_width] = x\n",
    "        return padded_x\n",
    "\n",
    "    def _check_parameters(self):\n",
    "        if self.groups <= 0:\n",
    "            raise ValueError('groups must be a positive integer')\n",
    "        if self.in_channels % self.groups != 0:\n",
    "            raise ValueError('in_channels should be divisible by groups')\n",
    "        if self.out_channels % self.groups != 0:\n",
    "            raise ValueError('out_channels should be divisible by groups')\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        batch_size, in_channels, in_height, in_width = x.size()\n",
    "        out_height = (in_height + 2 * self.padding[0] - self.dilation[0] * (self.kernel_size[0] - 1) - 1) // \\\n",
    "                     self.stride[0] + 1\n",
    "        out_width = (in_width + 2 * self.padding[1] - self.dilation[1] * (self.kernel_size[1] - 1) - 1) // self.stride[\n",
    "            1] + 1\n",
    "\n",
    "        if self.padding[0] > 0 or self.padding[1] > 0:\n",
    "            x = self.add_padding(x, self.padding[0])\n",
    "\n",
    "        out = torch.zeros(batch_size, self.out_channels, out_height, out_width)\n",
    "\n",
    "        for h in range(out_height):\n",
    "            for w in range(out_width):\n",
    "                h_start = h * self.stride[0]\n",
    "                h_end = h_start + self.kernel_size[0]\n",
    "                w_start = w * self.stride[1]\n",
    "                w_end = w_start + self.kernel_size[1]\n",
    "                receptive_field = x[:, :, h_start:h_end, w_start:w_end]\n",
    "\n",
    "                out[:, :, h, w] = torch.sum(\n",
    "                    receptive_field.unsqueeze(1) * self.weights.view(1, self.out_channels,\n",
    "                                                                     self.in_channels // self.groups,\n",
    "                                                                     *self.kernel_size),\n",
    "                    dim=(2, 3, 4)\n",
    "                ) + self.bias.view(1, self.out_channels)\n",
    "\n",
    "        self.last_input = x\n",
    "        return out\n",
    "\n",
    "\n",
    "# Max Pool Layer\n",
    "class MaxPool2d:\n",
    "    def __init__(self, \n",
    "                 kernel_size: int | Tuple[int, int],\n",
    "                 stride: int | Tuple[int, int]):\n",
    "        self.kernel_size = (kernel_size \n",
    "                            if isinstance(kernel_size, tuple) and len(kernel_size) == 2 \n",
    "                            else (kernel_size, kernel_size) \n",
    "                            if isinstance(kernel_size, int) else (2, 2))\n",
    "        self.stride = (stride \n",
    "                       if isinstance(stride, tuple) and len(stride) == 2 \n",
    "                       else (stride, stride) \n",
    "                       if isinstance(stride, int) else (2, 2))\n",
    "        self.kh, self.kw = self.kernel_size\n",
    "        self.sh, self.sw = self.stride\n",
    "\n",
    "    def prepare_submatrix(self, X: torch.Tensor):\n",
    "        B, C, ih, iw = X.shape\n",
    "        oh = (ih - self.kh) // self.sh + 1\n",
    "        ow = (iw - self.kw) // self.sw + 1\n",
    "        subM = X.unfold(2, self.kh, self.sh).unfold(3, self.kw, self.sw)\n",
    "        return subM\n",
    "\n",
    "    def forward(self, X: torch.Tensor):\n",
    "        self.X = X\n",
    "        subM = self.prepare_submatrix(X)\n",
    "        return subM.max(dim=-1).values.max(dim=-1).values\n",
    "\n",
    "    def add_padding(self,\n",
    "                    X: torch.Tensor,\n",
    "                    padding: int):\n",
    "        padding = tuple(repeat(padding, 4))\n",
    "        batch_size, in_channels, original_height, original_width = X.size()\n",
    "        padded_height = original_height + padding[0] + padding[1]\n",
    "        padded_width = original_width + padding[2] + padding[3]\n",
    "\n",
    "        padded_x = torch.zeros((batch_size, in_channels, padded_height, padded_width), dtype=X.dtype)\n",
    "        padded_x[:, :, padding[0]:padding[0] + original_height, padding[2]:padding[2] + original_width] = X\n",
    "        return padded_x\n",
    "\n",
    "    def prepare_mask(self,\n",
    "                     subM: torch.Tensor,\n",
    "                     kh: int,\n",
    "                     kw: int):\n",
    "        B, C, oh, ow, kh, kw = subM.shape\n",
    "        a = torch.reshape(subM, (-1, kh * kw))\n",
    "        idx = torch.argmax(a, dim=1)\n",
    "        b = torch.zeros_like(a)\n",
    "        b[torch.arange(b.shape[0]), idx] = 1\n",
    "        mask = b.view(B, C, oh, ow, kh, kw)\n",
    "        return mask\n",
    "\n",
    "    def mask_dXp(self, mask: torch.Tensor,\n",
    "                 Xp: torch.Tensor,\n",
    "                 dz: torch.Tensor,\n",
    "                 kh: int,\n",
    "                 kw: int):\n",
    "        dA = torch.einsum('i,ijk->ijk',\n",
    "                          dz.view(-1),\n",
    "                          mask.view(-1, kh, kw)).view(mask.shape)\n",
    "        B, C, ih, iw = Xp.shape\n",
    "        strides = (C * ih * iw, ih * iw, iw, 1)\n",
    "        strides = tuple(i * Xp.element_size() for i in strides)\n",
    "        dXp = torch.as_strided(dA, Xp.shape, strides)\n",
    "        return dXp\n",
    "\n",
    "    def maxpool_backprop(self,\n",
    "                         dz: torch.Tensor):\n",
    "        X = self.X\n",
    "        Xp = self.add_padding(X, self.kernel_size[0])\n",
    "        subM = self.prepare_submatrix(Xp)\n",
    "        B, C, oh, ow, kh, kw = subM.shape\n",
    "        B, C, ih, iw = Xp.shape\n",
    "\n",
    "        mask = self.prepare_mask(subM, kh, kw)\n",
    "        dXp = self.mask_dXp(mask, Xp, dz, kh, kw)\n",
    "        return dXp\n",
    "\n",
    "\n",
    "# Fully Connected Layer\n",
    "class FCLayer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = torch.randn(input_size, output_size, requires_grad=True) / input_size\n",
    "        self.biases = torch.zeros(output_size, requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.last_input_shape = x.shape\n",
    "        self.last_input = x.view(x.shape[0], -1)\n",
    "        return (self.last_input @ self.weights) + self.biases\n",
    "\n",
    "    def backward(self, d_L_d_out, lr):\n",
    "        d_L_d_weights = self.last_input.T @ d_L_d_out\n",
    "        d_L_d_biases = torch.sum(d_L_d_out, dim=0)\n",
    "\n",
    "        d_L_d_input = d_L_d_out @ self.weights.T\n",
    "        d_L_d_input = d_L_d_input.view(self.last_input_shape)\n",
    "\n",
    "        self.weights = self.weights - lr * d_L_d_weights\n",
    "        self.biases = self.biases - lr * d_L_d_biases\n",
    "\n",
    "        return d_L_d_input\n",
    "\n",
    "\n",
    "# Model\n",
    "class SimpleCNN:\n",
    "    def __init__(self):\n",
    "        self.conv1 = Conv2d(1, 8, 3)\n",
    "        self.pool = MaxPool2d(2, 2)\n",
    "        self.conv2 = Conv2d(8, 16, 3)\n",
    "        self.fc1 = FCLayer(16 * 5 * 5, 120)\n",
    "        self.fc2 = FCLayer(120, 84)\n",
    "        self.fc3 = FCLayer(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = relu(self.conv1.forward(x))\n",
    "        x = self.pool.forward(x)\n",
    "        x = relu(self.conv2.forward(x))\n",
    "        x = self.pool.forward(x)\n",
    "        x = relu(self.fc1.forward(x))\n",
    "        x = relu(self.fc2.forward(x))\n",
    "        x = self.fc3.forward(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Training function\n",
    "def train(model, criterion, optimizer, dataloader, num_epochs=5):\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (inputs, labels) in enumerate(dataloader):\n",
    "            outputs = model.forward(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}, Batch {i + 1}/{len(dataloader)}, Loss: {loss.item()}\")\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "# Optimizer placeholder\n",
    "class Optimizer:\n",
    "    def __init__(self, params, lr):\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.params:\n",
    "            if param.grad is not None:\n",
    "                param.grad.detach_()\n",
    "                param.grad.zero_()\n",
    "\n",
    "    def step(self):\n",
    "        for param in self.params:\n",
    "            if param.grad is not None:\n",
    "                param.data -= self.lr * param.grad\n",
    "\n",
    "\n",
    "# Instantiate the model and optimizer\n",
    "model = SimpleCNN()\n",
    "params = [model.conv1.weights, model.conv1.bias,\n",
    "          model.conv2.weights, model.conv2.bias,\n",
    "          model.fc1.weights, model.fc1.biases,\n",
    "          model.fc2.weights, model.fc2.biases,\n",
    "          model.fc3.weights, model.fc3.biases]\n",
    "optimizer = Optimizer(params, lr=0.001)\n",
    "\n",
    "# Create synthetic dataset\n",
    "def create_synthetic_data(num_samples=1000):\n",
    "    # Generate random images of size 28x28 with 1 channel (grayscale)\n",
    "    images = torch.randn(num_samples, 1, 28, 28)\n",
    "    # Generate random labels (10 classes)\n",
    "    labels = torch.randint(0, 10, (num_samples,))\n",
    "    return images, labels\n",
    "\n",
    "# Create DataLoader\n",
    "def get_dataloader(batch_size=32, num_samples=1000):\n",
    "    images, labels = create_synthetic_data(num_samples)\n",
    "    dataset = TensorDataset(images, labels)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return dataloader\n",
    "\n",
    "# Define the criterion\n",
    "criterion = cross_entropy_loss\n",
    "\n",
    "# Get the DataLoader\n",
    "train_loader = get_dataloader()\n",
    "\n",
    "# Training loop\n",
    "train(model, criterion, optimizer, train_loader, num_epochs=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a893aff0-a8da-4a62-ad2a-d176e4b124aa",
   "metadata": {},
   "source": [
    "## New Version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c1fa74-e528-4240-b202-02744e8f2255",
   "metadata": {},
   "source": [
    "### Maxpool2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6f483978-8aed-4012-b205-6f1237f68f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool2d:\n",
    "\n",
    "    def __init__(self, \n",
    "                 kernel_size: int|Tuple,\n",
    "                 stride: int|Tuple):\n",
    "        self.kernel_size = (kernel_size \n",
    "                            if isinstance(kernel_size, tuple) and len(kernel_size) == 2 \n",
    "                            else (kernel_size, kernel_size) \n",
    "                            if isinstance(kernel_size,int) else (2,2))\n",
    "        self.stride = (stride \n",
    "                            if isinstance(stride, tuple) and len(stride) == 2 \n",
    "                            else (stride, stride) \n",
    "                            if isinstance(stride,int) else (2,2))\n",
    "        self.kh , self.kw = self.kernel_size\n",
    "        self.sh, self.sw = self.stride\n",
    "\n",
    "    def prepare_submatrix(self, X: torch.Tensor):\n",
    "        B, C, ih, iw = X.shape\n",
    "        oh = (ih - self.kh) // self.sh + 1\n",
    "        ow = (iw - self.kw) // self.sw + 1\n",
    "        subM = X.unfold(2, self.kh, self.sh).unfold(3,self.kw, self.sw)\n",
    "        return subM\n",
    "\n",
    "    def forward(self, X:torch.Tensor):\n",
    "        self.X = X\n",
    "        subM = self.prepare_submatrix(X)\n",
    "        return subM.max(dim = -1).values.max(dim = -1).values\n",
    "\n",
    "    \n",
    "    def add_padding(self,\n",
    "                X: torch.Tensor,\n",
    "                padding: int):\n",
    "        padding = tuple(repeat(padding, 4))\n",
    "        batch_size, in_channels, original_height, original_width = X.size()\n",
    "        padded_height = original_height + padding[0] + padding[1]\n",
    "        padded_width = original_width + padding[2] + padding[3]\n",
    "    \n",
    "        padded_x = torch.zeros((batch_size, in_channels, padded_height, padded_width), dtype=X.dtype)\n",
    "        padded_x[:, :, padding[0]:padding[0] + original_height, padding[2]:padding[2] + original_width] = X\n",
    "        return padded_x\n",
    "\n",
    "    def prepare_mask(self,\n",
    "                     subM:torch.Tensor,\n",
    "                     kh:int,\n",
    "                     kw: int\n",
    "                    ):\n",
    "        B,C,oh,ow,kh,kw = subM.shape\n",
    "        print(torch.reshape(subM,(-1, kh * kw)))\n",
    "        # a = subM.view(-1, kh * kw)\n",
    "        a = torch.reshape(subM,(-1, kh * kw))\n",
    "        idx = torch.argmax(a ,dim=1)\n",
    "        b = torch.zeros_like(a)\n",
    "        b[torch.arange(b.shape[0]), idx] = 1\n",
    "        mask = b.view(B,C, oh,ow, kh,kw)\n",
    "        return mask\n",
    "\n",
    "    def mask_dXp(self, mask: torch.Tensor,\n",
    "                Xp: torch.Tensor,\n",
    "                dz: torch.Tensor,\n",
    "                kh: int,\n",
    "                kw: int):\n",
    "        dA = torch.einsum('i,ijk->ijk',\n",
    "                         dz.view(-1),\n",
    "                         mask.view(-1,kh,kw)).view(mask.shape)\n",
    "        B,C,ih,iw = Xp.shape\n",
    "        strides = (C * ih * iw, ih * iw, iw, 1)\n",
    "        strides = tuple(i * Xp.element_size() for i in strides)\n",
    "        dXp = torch.as_strided(dA, Xp.shape, strides)\n",
    "        return dXp\n",
    "\n",
    "    def parameters(Self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cef3ae-5aac-42e4-84ee-65797fda4c65",
   "metadata": {},
   "source": [
    "## Activations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "c2e6af08-0876-4044-a189-ee83f32b43b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "\n",
    "    def __call__(self, \n",
    "                 X: torch.Tensor):\n",
    "        return torch.clamp(X, min=0)\n",
    "\n",
    "    def backward(self, \n",
    "                 dZ:torch.Tensor):\n",
    "        return (dZ > 0).float\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "de6590ce-d4c9-491c-b7ca-9e7d31403d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2437, -1.3459,  0.0191, -0.3140],\n",
       "        [-0.8502, -1.6319,  1.1911, -2.5889],\n",
       "        [-0.3407, -0.5172, -1.7387, -0.4112],\n",
       "        [ 0.2187,  0.5901, -0.6600, -0.4221]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel = torch.randn(4,4)\n",
    "rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0f7e30b1-5cfe-40da-b143-1f5262a68589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0000, -0.0000, 0.0191, -0.0000],\n",
       "        [-0.0000, -0.0000, 1.1911, -0.0000],\n",
       "        [-0.0000, -0.0000, -0.0000, -0.0000],\n",
       "        [0.2187, 0.5901, -0.0000, -0.0000]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel * (rel > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "59d00647-4ea5-4484-bfd4-6a8d6bc5db58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0191, 0.0000],\n",
       "        [0.0000, 0.0000, 1.1911, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2187, 0.5901, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.clamp(rel, min=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ef2e8777-dcf8-46e0-b373-fa694af1916a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0.]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(rel > 0).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd35bf5-0f05-4318-83dd-97fae8655fcc",
   "metadata": {},
   "source": [
    "### Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "43fbfecd-7c38-46f6-97f6-98cb99133d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "\n",
    "    def __call__(self, \n",
    "                X: torch.Tensor, \n",
    "                dim: int):\n",
    "        X = X - torch.max(X, dim = 1, keepdims = True).values\n",
    "        sof = torch.exp(X)/torch.sum(torch.exp(X), dim = dim , keepdims = True)\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a76bde15-1666-47c8-aecc-9cfcda5aa42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Its not needeed  to calculate tghe gradients for the softmax since the softymax is being used in calculating the Cross Entropy Loss\n",
    "#the gradients from the CE loss is suffiecient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "68a11cda-407c-4575-8166-3d33180333f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5686, -1.1641,  1.7440],\n",
       "        [-0.7862, -1.7022,  0.4724]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4b703a37-87cc-4a47-a82a-bf13d34d2d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9029,  2.1909, -0.7375],\n",
      "        [ 1.5797,  1.1408,  0.9713]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2,3)\n",
    "print(x)\n",
    "x = x - torch.max(x, dim = 1, keepdims = True).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "177eccdf-ce44-4fa5-be77-566cfbcce06c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([[0.],\n",
       "        [0.]]),\n",
       "indices=tensor([[0],\n",
       "        [0]]))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(x, dim =1 , keepdim = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "cbe31d99-320b-4472-b404-eef16fb94f8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.0938,  0.0000, -2.9284],\n",
       "        [ 0.0000, -0.4390, -0.6085]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "483464f2-ace9-4740-95d8-c6f2618064dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0413, 0.9101, 0.0487],\n",
       "        [0.4569, 0.2945, 0.2486]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(x)/torch.sum(torch.exp(x), dim = 1 , keepdims = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "14be70a7-d01d-41d7-b1c3-a6899af5aea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax Output:\n",
      "tensor([[0.0900, 0.2447, 0.6652],\n",
      "        [0.2595, 0.7054, 0.0351]])\n",
      "Gradient wrt Input:\n",
      "tensor([[ 0.0218,  0.0836, -0.1054],\n",
      "        [ 0.1675, -0.1796,  0.0121]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class Softmax:\n",
    "    def __call__(self, x: torch.Tensor):\n",
    "        x = x - x.max(dim=1, keepdim=True).values\n",
    "        exp_x = torch.exp(x)\n",
    "        self.softmax_output = exp_x / exp_x.sum(dim=1, keepdim=True)\n",
    "        return self.softmax_output\n",
    "\n",
    "    def backward(self, dZ: torch.Tensor):\n",
    "        if len(dZ.shape) == 1:\n",
    "            dZ.unsqueeze_(dim=0)\n",
    "            \n",
    "        batch_size, num_classes = dZ.shape\n",
    "        dX = torch.empty_like(dZ)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            s = self.softmax_output[i].unsqueeze(1)\n",
    "            jacobian_matrix = torch.diagflat(s) - s @ s.T\n",
    "            dX[i] = dZ[i] @ jacobian_matrix\n",
    "\n",
    "        return dX\n",
    "\n",
    "# Example usage\n",
    "softmax = Softmax()\n",
    "\n",
    "# Forward pass\n",
    "x = torch.tensor([[1.0, 2.0, 3.0], [1.0, 2.0, -1.0]])\n",
    "output = softmax(x)\n",
    "print(\"Softmax Output:\")\n",
    "print(output)\n",
    "\n",
    "# Backward pass (assuming some gradient from the next layer)\n",
    "dZ = torch.tensor([[0.1, 0.2, -0.3], [0.4, -0.5, 0.1]])\n",
    "dX = softmax.backward(dZ)\n",
    "print(\"Gradient wrt Input:\")\n",
    "print(dX)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa620bc-cbc4-44b5-a4c2-8189e2fa7b7b",
   "metadata": {},
   "source": [
    " ### Loss - Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "660c58a2-3149-4767-8c1f-b89ffd34bd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss:\n",
    "\n",
    "    def __call__(self,\n",
    "                 y_pred: torch.Tensor,\n",
    "                 y_true: torch.Tensor\n",
    "                ):\n",
    "        n_samples = y_pred.shape[0]\n",
    "        log_likelihood = -torch.log(y_pred[range(n_samples), y_true])\n",
    "        return torch.sum(log_likelihood) / n_samples\n",
    "\n",
    "    def backward(self,\n",
    "                y_pred: torch.Tensor,\n",
    "                y_true: torch.Tensor\n",
    "                ):\n",
    "        n_samples = y_pred.shape[0]\n",
    "        softmax = Softmax()\n",
    "        grad = softmax(y_pred, dim=1)\n",
    "        grad[range(n_samples), y_true] -= 1\n",
    "        grad = grad / n_samples\n",
    "        return grad\n",
    "\n",
    "    def paramerters(self):\n",
    "        return []\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "537800ad-2bee-4f44-a2fb-48069b891ba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9969, 0.2697, 0.7932, 0.6488],\n",
       "        [0.0515, 0.4998, 0.1391, 0.5888],\n",
       "        [0.1619, 0.6078, 0.9070, 0.1058],\n",
       "        [0.3053, 0.7272, 0.0810, 0.2980],\n",
       "        [0.7713, 0.9617, 0.2650, 0.0125],\n",
       "        [0.3623, 0.2542, 0.2724, 0.4133],\n",
       "        [0.0827, 0.4158, 0.6465, 0.4369],\n",
       "        [0.0306, 0.0630, 0.8786, 0.2498],\n",
       "        [0.7035, 0.7723, 0.3581, 0.4975],\n",
       "        [0.8384, 0.5916, 0.9916, 0.5968]])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(10,4)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "7e54ba6a-93e2-400d-bcb0-188b0ab3d18e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 0, 0, 0, 0, 3, 2, 1, 0, 0])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.randint(0,4, (10,))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "d72ae4d5-a301-49fb-9fcb-edf94b5db0ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1278)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = CrossEntropyLoss()\n",
    "loss(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "bf3b828c-80c4-4abc-a12c-d9142f3803a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6488, 0.0515, 0.1619, 0.3053, 0.7713, 0.4133, 0.6465, 0.0630, 0.7035,\n",
       "        0.8384])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[range(10), y]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfa88c6-5d5e-4d40-9f9d-54bd4672a9d3",
   "metadata": {},
   "source": [
    "## Optimizer -SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "0f3062d4-7d8c-4de8-853b-5e75d0097dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional , List\n",
    "class OptimizerSG:\n",
    "\n",
    "    def __init__(self,\n",
    "                params: Optional[List],\n",
    "                lr : float = 0.1):\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self):\n",
    "        for param in self.params:\n",
    "            if param.grad is not None:\n",
    "                param.data -= self.lr * param.grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c53481b-dab7-4916-bc5f-834a5c6c24d1",
   "metadata": {},
   "source": [
    "## Flatten Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "a6886dee-a413-4a8d-b2e3-c3c9d467b4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten:\n",
    "\n",
    "    def __call__(self,\n",
    "                X: torch.Tensor):\n",
    "        self.X = X\n",
    "        self.out = X.view(X.shape[0], -1)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self,\n",
    "                 dZ: torch.Tensor):\n",
    "        dX = dZ.view(self.X.size)\n",
    "        return dX \n",
    "    \n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "65b95435-a3dc-47f4-8060-2df70abfdd29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 4])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43ae031-0b70-443d-80e8-419169d23889",
   "metadata": {},
   "source": [
    "## Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "617068ff-e01b-4d1c-aecd-5f5b21ace20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "    def __init__(self, \n",
    "                layers: List):\n",
    "        self.layers = layers\n",
    "\n",
    "    def __call__(self,\n",
    "                X: torch.Tensor):\n",
    "        for layer in self.layers:\n",
    "            X = layer(X)\n",
    "        self.out = X\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.parameters for p in layer.parameters()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0b6bc4-fa34-402a-a63f-e482a5288c68",
   "metadata": {},
   "source": [
    "## Convultion2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "d462c777-7cb4-425f-bd03-03d7490b8d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Conv2d Layer\n",
    "class Conv2d:\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 kernel_size: int,\n",
    "                 stride: int = 1,\n",
    "                 padding: int = 0,\n",
    "                 dilation: int = 1,\n",
    "                 groups: int = 1):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.groups = groups\n",
    "        self._check_parameters()\n",
    "        self._n_tuple()\n",
    "        self.weights, self.bias = self.initialise_weights()\n",
    "\n",
    "    def _n_tuple(self):\n",
    "        self.kernel_size = (self.kernel_size, self.kernel_size)\n",
    "        self.stride = (self.stride, self.stride)\n",
    "        self.padding = (self.padding, self.padding)\n",
    "        self.dilation = (self.dilation, self.dilation)\n",
    "\n",
    "    def initialise_weights(self):\n",
    "        return (torch.randn(self.out_channels, self.in_channels // self.groups, *self.kernel_size, requires_grad=True),\n",
    "                torch.zeros(self.out_channels, requires_grad=True))\n",
    "\n",
    "    def add_padding(self, x: torch.Tensor, padding: int):\n",
    "        padding = tuple(repeat(padding, 4))\n",
    "        batch_size, in_channels, original_height, original_width = x.size()\n",
    "        padded_height = original_height + padding[0] + padding[1]\n",
    "        padded_width = original_width + padding[2] + padding[3]\n",
    "\n",
    "        if (self.padded_height and self.padded_width) is None:\n",
    "            self.padded_height, self.padded_width = padded_height, padded_width\n",
    "\n",
    "        padded_x = torch.zeros((batch_size, in_channels, padded_height, padded_width), dtype=x.dtype)\n",
    "        padded_x[:, :, padding[0]:padding[0] + original_height, padding[2]:padding[2] + original_width] = x\n",
    "        return padded_x\n",
    "\n",
    "    def _check_parameters(self):\n",
    "        if self.groups <= 0:\n",
    "            raise ValueError('groups must be a positive integer')\n",
    "        if self.in_channels % self.groups != 0:\n",
    "            raise ValueError('in_channels should be divisible by groups')\n",
    "        if self.out_channels % self.groups != 0:\n",
    "            raise ValueError('out_channels should be divisible by groups')\n",
    "\n",
    "    def __call__(self, x: torch.Tensor):\n",
    "        self.X = x\n",
    "        self.ih, self.iw = self.X.shape[-2], self.X.shape[-1]\n",
    "        batch_size, in_channels, in_height, in_width = x.size()\n",
    "        out_height = (in_height + 2 * self.padding[0] - self.dilation[0] * (self.kernel_size[0] - 1) - 1) // \\\n",
    "                     self.stride[0] + 1\n",
    "        out_width = (in_width + 2 * self.padding[1] - self.dilation[1] * (self.kernel_size[1] - 1) - 1) // self.stride[\n",
    "            1] + 1\n",
    "\n",
    "        if self.padding[0] > 0 or self.padding[1] > 0:\n",
    "            x = self.add_padding(x, self.padding[0])\n",
    "\n",
    "        out = torch.zeros(batch_size, self.out_channels, out_height, out_width)\n",
    "\n",
    "        for h in range(out_height):\n",
    "            for w in range(out_width):\n",
    "                h_start = h * self.stride[0]\n",
    "                h_end = h_start + self.kernel_size[0]\n",
    "                w_start = w * self.stride[1]\n",
    "                w_end = w_start + self.kernel_size[1]\n",
    "                receptive_field = x[:, :, h_start:h_end, w_start:w_end]\n",
    "\n",
    "                out[:, :, h, w] = torch.sum(\n",
    "                    receptive_field.unsqueeze(1) * self.weights.view(1, self.out_channels,\n",
    "                                                                     self.in_channels // self.groups,\n",
    "                                                                     *self.kernel_size),\n",
    "                    dim=(2, 3, 4)\n",
    "                ) + self.bias.view(1, self.out_channels)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def prepare_subMatrix(self, X, Kh, Kw, s):\n",
    "        B, C, ih, iw = X.shape\n",
    "        sh, sw = s\n",
    "\n",
    "        Oh = (ih - Kh) // sh + 1\n",
    "        Ow = (iw - Kw) // sw + 1\n",
    "\n",
    "        strides = (C * ih * iw, iw * ih, iw * sh, sw, iw, 1)\n",
    "        subM = torch.as_strided(X,\n",
    "                                shape=(B, C, Oh, Ow, Kh, Kw),\n",
    "                                strides=strides\n",
    "                                )\n",
    "        return subM\n",
    "\n",
    "    def padding_backward(self,\n",
    "                         dXp: torch.Tensor):\n",
    "\n",
    "        B, C, ih, iw = self.X.shape\n",
    "        dX = dXp[:, :, self.padded_height:ih, self.padded_width:iw]\n",
    "        return dX\n",
    "\n",
    "    def convolve(self, X: torch.Tensor,\n",
    "                 K: torch.Tensor,\n",
    "                 s: Tuple = (1, 1),\n",
    "                 mode: str = 'back'):\n",
    "\n",
    "        F, Kc, Kh, Kw = K.shape\n",
    "        subM = self.prepare_subMatrix(X, Kh, Kw, s)\n",
    "\n",
    "        if mode == 'front':\n",
    "            return torch.einsum('fckl,mcijkl->mfij', K, subM)\n",
    "        elif mode == 'back':\n",
    "            return torch.einsum('fdkl,mcijkl->mdij', K, subM)\n",
    "        elif mode == 'param':\n",
    "            return torch.einsum('mfkl,mcijkl->fcij', K, subM)\n",
    "\n",
    "    def dz_D_dx(self,\n",
    "                dZ: torch.Tensor,\n",
    "                ih: int,\n",
    "                iw: int):\n",
    "        _, _, Hd, Wd = dZ.shape\n",
    "        ph = ih - Hd + self.kernel_size[0] - 1\n",
    "        pw = iw - Wd + self.kernel_size[0] - 1\n",
    "\n",
    "        dZ_Dp = self.add_padding(dZ, ph)\n",
    "\n",
    "        # Rotate the Kernel by 180 degrees\n",
    "        k_rotated = self.weights[:, :, ::-1, ::-1]\n",
    "\n",
    "        # convolve w.r.t k_rotated\n",
    "        dXp = self.convolve(dZ_Dp, k_rotated, mode='back')\n",
    "\n",
    "        dX = self.padding_backward(dXp)\n",
    "        return dX\n",
    "\n",
    "    def backward(self,\n",
    "                 dZ: torch.Tensor):\n",
    "        Xp: torch.Tensor = self.add_padding(self.X, self.padding)\n",
    "        B, C, ih, iw = Xp.shape\n",
    "\n",
    "        # dZ -> dZ_D_dX\n",
    "        dX = self.dz_D_dx(dZ, ih, iw)\n",
    "\n",
    "        # gradient dK\n",
    "        _, _, Hd, Wd = dZ.shape\n",
    "        ph = self.ih - Hd - self.kernel_size[0] + 1\n",
    "        pw = self.iw - Wd - self.kernel_size[0] + 1\n",
    "\n",
    "        dZ_Dp = self.add_padding(dZ, padding=ph)\n",
    "\n",
    "        self.dweights = self.convolve(Xp, dZ_Dp, mode='param')\n",
    "\n",
    "        # gradient db\n",
    "        self.dbias = dZ.sum(0)\n",
    "\n",
    "        return dX\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weights] + ([] if self.bias is None else [self.bias])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "f995e9c8-68ab-42bd-a9ef-2236eb5dc6ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[-0.3372, -0.1607],\n",
       "           [-0.0424,  0.2792]],\n",
       " \n",
       "          [[ 0.0431,  0.5649],\n",
       "           [-1.8187,  0.3276]],\n",
       " \n",
       "          [[ 0.1595,  0.3155],\n",
       "           [-0.4101, -0.5286]]],\n",
       " \n",
       " \n",
       "         [[[ 0.7688, -1.4299],\n",
       "           [-1.6796, -1.0166]],\n",
       " \n",
       "          [[ 0.7357, -0.2954],\n",
       "           [ 0.0655,  0.7476]],\n",
       " \n",
       "          [[-1.7967, -0.3187],\n",
       "           [ 0.3662, -0.2843]]],\n",
       " \n",
       " \n",
       "         [[[ 0.3952,  0.3623],\n",
       "           [-1.9322,  0.1265]],\n",
       " \n",
       "          [[-0.6430, -0.7050],\n",
       "           [-1.0528, -0.3704]],\n",
       " \n",
       "          [[-0.9510, -0.3699],\n",
       "           [-1.1384, -1.4173]]],\n",
       " \n",
       " \n",
       "         [[[-0.3879,  0.5395],\n",
       "           [ 0.5747, -0.1485]],\n",
       " \n",
       "          [[ 0.2446, -1.0037],\n",
       "           [ 1.1414,  0.6541]],\n",
       " \n",
       "          [[ 0.5049,  1.0136],\n",
       "           [-1.0679, -1.5061]]],\n",
       " \n",
       " \n",
       "         [[[ 1.8986, -0.1898],\n",
       "           [-0.5832, -1.3480]],\n",
       " \n",
       "          [[-0.4289,  0.0443],\n",
       "           [-0.5146,  1.4133]],\n",
       " \n",
       "          [[ 0.2470,  0.1782],\n",
       "           [ 0.3316, -0.5827]]]], requires_grad=True),\n",
       " tensor([0., 0., 0., 0., 0.], requires_grad=True))"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = Conv2d(3, 5, 2)\n",
    "conv.weights, conv.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "ce2b912e-5d23-499a-ab6b-37ebad396132",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 1.18893149, -0.9696258 ,  1.06130319,  0.31716785,\n",
       "          -0.61831145],\n",
       "         [-0.50221894, -0.27220749,  1.38423068, -0.21969117,\n",
       "           0.06406922],\n",
       "         [ 0.00484239,  1.70762968,  0.39114741,  0.04030697,\n",
       "           0.30507207],\n",
       "         [-0.42158428, -1.44251038, -0.03865119, -0.28017898,\n",
       "          -0.19429141],\n",
       "         [ 0.30450748,  0.78276913,  0.00453206,  0.29052367,\n",
       "           1.36977019]]]])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.randn(1,1,5,5)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59205e9b-70d3-40bc-a6dc-92e497953f72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "6ea04006-831f-45cf-8a42-0e0a62647255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 1.18893149  0.         -0.9696258   0.          1.06130319\n",
      "     0.          0.31716785  0.         -0.61831145]\n",
      "   [-0.50221894  0.         -0.27220749  0.          1.38423068\n",
      "     0.         -0.21969117  0.          0.06406922]\n",
      "   [ 0.00484239  0.          1.70762968  0.          0.39114741\n",
      "     0.          0.04030697  0.          0.30507207]\n",
      "   [-0.42158428  0.         -1.44251038  0.         -0.03865119\n",
      "     0.         -0.28017898  0.         -0.19429141]\n",
      "   [ 0.30450748  0.          0.78276913  0.          0.00453206\n",
      "     0.          0.29052367  0.          1.36977019]]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[[ 1.18893149,  0.        , -0.9696258 ,  0.        ,\n",
       "           1.06130319,  0.        ,  0.31716785,  0.        ,\n",
       "          -0.61831145],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ],\n",
       "         [-0.50221894,  0.        , -0.27220749,  0.        ,\n",
       "           1.38423068,  0.        , -0.21969117,  0.        ,\n",
       "           0.06406922],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ],\n",
       "         [ 0.00484239,  0.        ,  1.70762968,  0.        ,\n",
       "           0.39114741,  0.        ,  0.04030697,  0.        ,\n",
       "           0.30507207],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ],\n",
       "         [-0.42158428,  0.        , -1.44251038,  0.        ,\n",
       "          -0.03865119,  0.        , -0.28017898,  0.        ,\n",
       "          -0.19429141],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ],\n",
       "         [ 0.30450748,  0.        ,  0.78276913,  0.        ,\n",
       "           0.00453206,  0.        ,  0.29052367,  0.        ,\n",
       "           1.36977019]]]])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def dilate2D(X, Dr=(1,1)):\n",
    "    dh, dw = Dr # Dilate rate\n",
    "    m, C, H, W = X.shape\n",
    "    Xd = np.insert(arr=X, obj=np.repeat(np.arange(1,W), dw-1), values=0, axis=-1)\n",
    "    print(Xd)\n",
    "    Xd = np.insert(arr=Xd, obj=np.repeat(np.arange(1,H), dh-1), values=0, axis=-2)\n",
    "    return Xd\n",
    "\n",
    "dilate2D(x, (2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d7acc8-cbf0-4dec-9593-0a9ca26d88be",
   "metadata": {},
   "source": [
    "## Fully Connected Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf18fe3-f707-43c9-886f-c8483d99950d",
   "metadata": {},
   "source": [
    "### Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "cbc37285-9225-4340-8729-8c8dd05d0e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, \n",
    "                fan_in: int,\n",
    "                fan_out : int,\n",
    "                bias = True):\n",
    "        self.weight = torch.randn((fan_in, fan_out)) // fan_in ** 0.5\n",
    "        self.bias = torch.randn(fan_out) if bias else None\n",
    "\n",
    "    def __call__(self, \n",
    "                X: torch.Tensor):\n",
    "\n",
    "        self.out = X @ self.weight\n",
    "        if self.bias is not None:\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "e0d75c3a-b34b-4e41-8877-d16446cef5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "li = Linear(3,10)\n",
    "x = torch.randn(10,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "02c517f1-517f-4278-8e25-04ef6fa6a3f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.3764, -0.7310,  0.6046],\n",
       "         [ 0.0692, -1.4089,  0.2280],\n",
       "         [ 0.3872, -0.2472,  1.0910],\n",
       "         [-1.2647,  0.5164,  1.8031],\n",
       "         [ 1.9894, -0.3805,  0.8868],\n",
       "         [ 0.1882, -0.8929, -1.8867],\n",
       "         [-1.6680, -0.1485,  0.4404],\n",
       "         [ 0.8268,  0.2494,  0.0408],\n",
       "         [ 0.0881,  0.0684,  1.7387],\n",
       "         [ 0.5513, -0.0030, -0.3670]]),\n",
       " tensor([[ 0.0577,  2.1041,  1.1866,  0.5189, -0.1753,  0.6206, -1.9982, -1.7773,\n",
       "          -1.7881, -1.9033],\n",
       "         [ 1.1122,  2.7820,  3.6027,  1.1968,  0.2013,  0.6206, -1.3144, -1.7773,\n",
       "          -0.4264, -1.5268],\n",
       "         [-0.9125,  1.6202, -0.7646,  0.0351, -0.6616,  0.6206, -2.4953, -1.7773,\n",
       "          -2.7691, -2.3897],\n",
       "         [-2.3882,  0.8566, -2.0642, -0.7285, -1.3738,  0.6206, -1.5555, -1.7773,\n",
       "          -2.5930, -3.1018],\n",
       "         [-0.5750,  1.7536, -1.6919,  0.1684, -0.4575,  0.6206, -3.8934, -1.7773,\n",
       "          -4.0338, -2.1855],\n",
       "         [ 2.7109,  2.2660,  6.6812,  0.6808,  2.3160,  0.6206,  0.6814, -1.7773,\n",
       "           1.0533,  0.5879],\n",
       "         [-0.3606,  1.5216,  2.3944, -0.0636, -0.0111,  0.6206,  0.2105, -1.7773,\n",
       "          -0.1620, -1.7391],\n",
       "         [-0.3589,  1.1236, -0.0972, -0.4615,  0.3885,  0.6206, -1.8848, -1.7773,\n",
       "          -2.6552, -1.3396],\n",
       "         [-1.8758,  1.3047, -2.3921, -0.2805, -1.3094,  0.6206, -2.8439, -1.7773,\n",
       "          -3.4333, -3.0374],\n",
       "         [ 0.3013,  1.3760,  1.4988, -0.2091,  0.7963,  0.6206, -1.2015, -1.7773,\n",
       "          -1.7195, -0.9318]]))"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, li(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "a2282d86-35d2-4495-b399-cb08ec5abd86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  0., -1.,  0.,  0.,  0., -1.,  0., -1.,  0.],\n",
       "        [-1., -1., -2., -1.,  0.,  0.,  0.,  0., -1.,  0.],\n",
       "        [-1.,  0., -2.,  0., -1.,  0., -1.,  0., -1., -1.]])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "cbb6ddea-9dd5-458b-9561-40009b65b472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0687,  1.3730,  1.3101, -0.2121,  0.4293,  0.6206, -1.0171, -1.7773,\n",
       "        -1.5381, -1.2987])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1126a7cb-0f4f-4d46-a1ca-3edadcdb0528",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
